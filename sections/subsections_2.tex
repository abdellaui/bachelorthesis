\subsection{Künstliche neuronale Netzwerke}
\label{sec:KNN}
Die künstliche neuronale Netze sind ein Forschungsgebiet der künstlichen Intelligenz und imitieren die Beschaffenheit natürlicher neuronaler Netze, um komplexe Probleme zu lösen. Inspiriert von ihren biologischen Vorbildern\footnote{das Nervensystem eines komplexen Lebewesens; z.B. des Menschen}, vernetzen künstliche neuronale Netzwerke künstliche Neurone miteinander \cite{johnsonCS231nConvolutionalNeural}. Dabei kann die Verbindung unidirektional (\textit{feedforward}) oder bidirektional (\textit{feedback}) sein \cite{goodfellowDeepLearning2016}. 

Bei einem \textit{feedforward Netzwerk} werden die Daten im Netz immer vorwärts übertragen. Ein \textit{feedback Netzwerk}, auch bekannt als \textit{Recurrent Neural Networks}, kann dagegen Daten rückwärts sowie in einer Schleife zum selben Neuron übergeben \cite{goodfellowDeepLearning2016}. Da feedback Netzwerke keinen Einsatz in dieser Arbeit haben, ist im weiteren Verlauf bei einem Netzwerk immer ein feedforward Ansatz gemeint. 

In diesem Kapitel wird als Nächstes ein künstliches Neuron definiert und anschließend ein feedforward Netzwerk beschrieben. Convolutional Neural Networks sind eine besondere Art von künstlichen neuronalen Netzen und bilden einen sehr wichtigen Bestandteil dieser Arbeit. Daher werden den CNNs ein eigener Abschnitt gewidmet (s. Abschn. \ref{sec:CNN}).


\subsubsection{Künstliches Neuron}
Ein einzelnes Neuron erhält einen Inputsignal auf mehreren Kanälen und löst erst ein Signal (\textit{output}) aus, falls die gewichtete Summe des Inputs einen gewissen Schwellenwert erreicht \cite{johnsonCS231nConvolutionalNeural}. Abbildung \ref{fig:neuron} stellt eine beispielhafte Visualisierung eines künstlichen Neurons dar.

Ein künstliches Neuron mit der Inputgröße $M$ ist mathematisch die nicht-lineare Funktion $y : \mathbb{R}^M \mapsto \mathbb{R}$ mit den Parametern $x$ als Input, $w$ als Gewichtsvektor, $b$ als ein \textit{Bias}, $\phi$ als eine nicht-lineare \textit{Aktivierungsfunktion} \cite{johnsonCS231nConvolutionalNeural}:
\begin{equation}
	\label{eq:neuron}
	y(x)=\phi\left(\sum_{m=1}^{M} w_{i} x_{i} + b\right) = \phi(W^Tx+b)
\end{equation}


\begin{figure}
	\centering
	\includegraphics[width=0.8\textwidth]{images/ann_conv/neuron.png}
	\caption{Visualisierung eines künstlichen Neurons definiert nach der Gleichung \ref{eq:neuron}. Dieses Neuron summiert das Produkt des Inputvektors $x$ mit den jeweiligen Gewichten $w$ und addiert einen Bias $b$. Durch die Summe erzeugt die Aktivierungsfunktion $\phi$ das Output $y$ des Neurons. Abbildung \ref{fig:relu} zeigt ein Beispiel für eine Aktivierungsfunktion. Entnommen aus \cite{deoliveiraSystemBasedArtificial2017}. }
	\label{fig:neuron}
\end{figure}

\subsubsection{Feedforward Neural Networks}
\label{sec:feedforwardNN}
Künstliche Neuronen können zu einer Schicht (\textit{Layer}) zusammengeführt werden. Die Verbindung solcher Schichten bildet ein neuronales Netzwerk.
Bei einem feedforward (\textit{fully-connected}) Netzwerk übergibt jedes Neuron aus der Schicht $l$ den jeweiligen Output $y_{l}$ an jedem Neuron der Schicht $l+1$ weiter. Ebenso sind die Neuronen aus der gleichen Schicht nicht untereinander verbunden \cite{goodfellowDeepLearning2016}.
Die Schicht $l$ eines feedforward Netzwerks operiert somit auf das Ouput $y_{l-1}$ und stellt die nicht-lineare Funktion $f_l : \mathbb{R}^{M_{l-1}} \mapsto \mathbb{R}^{M_l}$ dar \cite{bauckhageInformedMachineLearning}:
\begin{equation}
\label{eq:layer}
y_l = f_l(y_{l-1}) = \phi(W^T_lx_{l-1}+b_{l})
\end{equation}


Die erste Schicht eines Netzwerks wird als Input-, die letzte Schicht als Ouput Layer bezeichnet. Alle Schichten dazwischen sind Hidden Layer \cite{goodfellowDeepLearning2016}. Der Ouput Layer liefert zugleich auch das Ergebnis eines Netzwerks. Deswegen haben die Neuronen des Ouput Layers grundsätzlich keine Aktivierungsfunktion \cite{johnsonCS231nConvolutionalNeural}.

Die Tiefe (\textit{depth}) eines Netzwerks ist gegeben durch die Anzahl der Layer\footnote{der Input Layer ist ausgeschlossen} und die Breite (\textit{width}) eines Layers wird durch die Anzahl der Neuronen bestimmt \cite{goodfellowDeepLearning2016}. 
Abbildung \ref{fig:neural_net} illustriert ein feedforward neuronales Netz als einen azyklischen Graphen.

Ziel eines KNNs ist es, eine Funktion $f^*$ zu approximieren, die einen Input $x$ auf einen Output $y$ abbildet. Durch das Output $y$ kann das Input $x$ klassifiziert oder anhand dessen ein Wert regressiert werden. Sei $y = f(x; \theta)$ eine derartige Funktion, dann besetzt ein KNN die Werte des $\theta$ Parameters mit eines der besten Approximation von $f^*$. Der Parameter $\theta$ stellt hierbei die Gewichte dar, die erlernt werden sollen \cite{goodfellowDeepLearning2016}. 
Das Lernen ist die strategische Anpassung der Gewichte über Input-Output Paare (\textit{Trainingsdaten}) und findet grundsätzlich durch ein \textit{Backpropagation}-Verfahren statt \cite{goodfellowDeepLearning2016}. 

Die Funktion $y = f(x; \theta)$ bildet sich aus den Funktionen der im Netzwerk vorhandenen Schichten (s. Gleichung \ref{eq:layer}) und kann bei einer Tiefe $L$ repräsentiert werden als die folgende Funktion $f : \mathbb{R}^{M_0} \mapsto \mathbb{R}^{M_L}$ \cite{goodfellowDeepLearning2016, bauckhageInformedMachineLearning}:
 
\begin{equation}
\label{eq:network}
y = f(x; \theta)= f_L(...f_2(f_1(x))) = (f_L * ... * f_2 * f_1)(x)
\end{equation}


\begin{figure}
	\centering
	\includegraphics[width=0.7\textwidth]{images/ann_conv/neural_net.png}
	\caption{Ein feedforward neuronales Netz mit der Tiefe 3, bestehend aus einem Input Layer der Breite 3, aus zwei Hidden Layer der Breite 4 und einem Output Layer der Breite 1. Mit der Gleichung \ref{eq:network} lässt sich dieses Netzwerk als die Funktion $f : \mathbb{R}^{3} \mapsto \mathbb{R}^{1}$ mit $f(x)=f_3(f_2(f_1(x)))$ darstellen. }
	\label{fig:neural_net}
\end{figure}

\subsection{Convolutional Neural Networks}
\label{sec:CNN}
Einfache neuronale Netze, wie sie in Abschnitt \ref{sec:KNN} beschrieben werden, arbeiten auf einen Inputvektor $x \in \mathbb{R}^{M}$. Im Vergleich dazu arbeiten CNNs auf einem drei-dimensionalen Inputvolumen $x \in \mathbb{R}^{width} \times \mathbb{R}^{heigth} \times \mathbb{R}^{depth}$. CNNs werden hauptsächlich in einem Kontext von Bildern eingesetzt. Dabei stellt z.B. ein 32 $\times$ 32 RGB-Bild einen Volumen von 32 $\times$ 32 $\times$ 3 dar \cite{johnsonCS231nConvolutionalNeurala}.

\begin{figure}
	\centering
	\includegraphics[width=0.95\textwidth]{images/ann_conv/lenet5.png}
	\caption{Ein Beispiel für ein Convolutional Neural Network. Es wird die LeNet-5 Architektur abgebildet. Entnommen aus \cite{lecunGradientBasedLearningApplied1998}}
	\label{fig:lenet5}
\end{figure}


Angefangen mit der LeNet-5 \cite{lecunGradientBasedLearningApplied1998} Architektur, setzt sich typischerweise ein Convolutional Neural Network aus einer Sequenz von unterschiedlichen Layer-Arten zusammen \cite{szegedyGoingDeeperConvolutions2015, johnsonCS231nConvolutionalNeurala}. Im weiteren Verlauf dieses Kapitels werden die Arten der Layer beschrieben. Abbildung \ref{fig:lenet5} illustriert die LeNet-5 Architektur als Beispiel für ein CNN. Tabelle \ref{tab:layer_param} gibt eine Übersicht der Layer und ihrer Parameter an.

\begin{table}[bh]
	\centering
	\caption{Übersicht der Parameter und Hyperparameter der Layer eines Convolutional Neural Networks. Die Parameter werden während der Trainingsphase optimiert und Hyperparameter werden zuvor fest definiert \cite{yamashitaConvolutionalNeuralNetworks2018}. }
	\begin{tabularx}{1.0\textwidth}{X X X}
		\textbf{Art des Layers} & \textbf{Parameter} & \textbf{Hyperparameter}\\
		\hline
		Convolutional Layer & Filter & \makecell[tl]{
			Filtergröße\\
			Anzahl der Filter (Tiefe)\\
			Stride\\
			Padding\\
			Aktivierungsfunktion
		}\\
		\hline
		Pooling Layer & \textit{keine} & \makecell[tl]{
			Pooling Methode\\
			Filtergröße\\
			Stride\\
			Padding
		}\\
		\hline
		Fully-Connected Layer & Gewichte & \makecell[tl]{
			Anzahl der Gewichte\\
			Aktivierungsfunktion
		}\\
		\hline
	\end{tabularx}
	\label{tab:layer_param}
\end{table}

\subsubsection{Convolutional Layer}
Der Convolutional Layer ist der Hauptbestandteil eines CNNs, der die Kombination aus einer Convolution Operation und einer Aktivierungsfunktion ist \cite{yamashitaConvolutionalNeuralNetworks2018}.

Die Convolution Operation basiert auf die mathematische Faltung und wird typischerweise in CNNs mit Verzicht auf die Kommutativität der anliegenden diskreten Kreuzkorrelation gleichgesetzt \cite{goodfellowDeepLearning2016}. Abbildung \ref{fig:convolution_layer} illustriert eine beispielhafte Convolution Operation.

Sei $I$ ein 2D-Input, $K$ ein 2D-Filter der Größe $M \times N$, $S$ die Schrittweite (\textit{Stride}) und $O$ das Output mit der Größe $X \times Y$, dann ist die Convolution Operation mathematisch definiert als \cite{goodfellowDeepLearning2016}:
\begin{equation}
	\label{eq:convolution}
	O(x,y) = (I * K)(x,y) = \sum_{m}\sum_{n}I(S_x \cdot x+m, S_y \cdot y+n)K(m,n)
\end{equation}
\begin{figure}
	\centering
	\includegraphics[width=0.4\textwidth]{images/ann_conv/ReLU.png}
	\caption{Ein Beispiel für eine Aktivierungsfunktion. Die \textit{Rectified-Liniear-Unit} (ReLU) Aktivierungsfunktion wird typischerweise in CNNs eingesetzt und ist mathematisch definiert als: $f(x) = max(0,x)$ \cite{goodfellowDeepLearning2016} }
	\label{fig:relu}
\end{figure}

Die Randbehandlung des Inputvolumens wird \textit{Padding} genannt. Es gibt eine Reihe von Padding Methoden. CNNs setzen grundsätzlich das \textit{Zero-Padding} Verfahren ein. Das Zero-Padding Verfahren erweitert den Rand des Inputvolumens um eine beliebige Breite und Höhe mit Nullen \cite{johnsonCS231nConvolutionalNeurala}. 

Die Werte des 2D-Filters stellen die Gewichte dar und werden in der Trainingsphase optimiert. Ein Convolutional Layer kann aus mehreren 2D-Filtern der gleichen Größe bestehen. Die Convolution Operation wird je 2D-Filter unabhängig auf die Schichten des Inputvolumens ausgeführt \cite{johnsonCS231nConvolutionalNeurala}. Das Output des Convolution Operation wird auch \textit{Feature-Map} genannt. Die Tiefe des Feature-Maps ist gegeben durch die Anzahl der 2D-Filter \cite{yamashitaConvolutionalNeuralNetworks2018}.

Prinzipiell wird das Output des Convolutional Layers durch die Übergabe des Feature-Maps an eine Aktivierungsfunktion bestimmt. Abbildung \ref{fig:relu} stellt eine Aktivierungsfunktion dar.

\begin{figure}
	\centering
	\includegraphics[width=0.7\textwidth]{images/ann_conv/convolution_layer.png}
	\caption{Ein Beispiel für die Convolution Operation mit 3 Filtern der Größe 3 $\times$ 3, je einem Stride von 1 und keinem Padding. Ein Filter bewegt sich entlang des gesamten Inputs mit der Schrittweite (\textit{Stride}) 1 und bildet die Summe der elementweise multiplizierten Werte. Die Summe wird dann im Feature-Map an die korrespondierende Position geschrieben. Dieser Vorgang wiederholt sich für jedes Filter \cite{yamashitaConvolutionalNeuralNetworks2018}. Abbildung basiert auf \cite{yamashitaConvolutionalNeuralNetworks2018}.}
	\label{fig:convolution_layer}
\end{figure}

\subsubsection{Pooling Layer}
Auf einem Convolutional Layer folgt i.d.R. ein Pooling Layer. Pooling Layer reduzieren die Größe eines Inputvolumens und verringern somit die Anzahl der erlernbaren Parameter \cite{johnsonCS231nConvolutionalNeurala}. Daher werden sie in der Literatur oft auch als \textit{Supsampling} Layer bezeichnet. Die Pooling Operation wird auf jeder Schicht der Eingabe ausgeführt \cite{johnsonCS231nConvolutionalNeurala}. Die meist verbreitete Pooling Methode ist die \textit{Max-Pooling}. Beim Max-Pooling iteriert ein Filter einer bestimmten Größe mit durch den Stride gegebener Schrittweite über das Inputvolumen und extrahiert das Maximum im aktuellen Filterbereich. Das Maximum wird für die weitere Berechnung beibehalten und die restlichen Werte werden verworfen
 \cite{johnsonCS231nConvolutionalNeurala}. In dieser Arbeit wird neben der Max-Pooling Operation auch die \textit{Average-Pooling} Operation eingesetzt. Das Average-Pooling behält den Durchschnittswert im aktuellen Filterbereich anstatt dem Maximum \cite{johnsonCS231nConvolutionalNeurala}.
 
 Im Vergleich zu einem Convolutional Layer wird ein Pooling Layer nur aus den Hyperparametern definiert und bleibt daher statisch \cite{yamashitaConvolutionalNeuralNetworks2018}. Abbildung \ref{fig:pooling_layer} zeigt eine beispielhafte Ausführung einer Max-Pooling Operation. Tabelle \ref{tab:layer_param} listet die Hyperparameter eines Pooling Layers auf.
 \begin{figure}
	\centering
	\includegraphics[width=0.55\textwidth]{images/ann_conv/max_pool.png}
	\caption{Ein Beispiel für eine Max-Pooling Operation mit einer Filtergröße von 2 $\times$ 2, einem Stride von 2 und keinem Padding. In diesem Beispiel wird der Input in 2 $\times$ 2 Bereiche unterteilt und der Maximum jedes Bereiches als Output berechnet. Es werden die markantesten Werte einer Nachbarschaft behalten und der Rest verworfen. Diese Operation führt zu einer Reduzierung der Inputgröße um den Faktor 2. Entnommen aus \cite{yamashitaConvolutionalNeuralNetworks2018}.}
	\label{fig:pooling_layer}
\end{figure}



\subsubsection{Fully-Connected Layer}
Nach einer Periode von Convolution- und Pooling Layer folgt meist ein \textit{fully-connected} (FC) Layer, auch bekannt als \textit{Dense Layer}. Dieser Layer folgt wie in Abschnitt \ref{sec:feedforwardNN} beschrieben dem gleichen Konzept der feedforward Neural Networks. Das Output dieses Layers wird häufig einer weiteren Aktivierungsfunktion übergeben \cite{yamashitaConvolutionalNeuralNetworks2018} und prinzipiell wird dadurch das Output des CNNs bestimmt.

\subsection{Bekannte CNN Modelle}
Es existiert eine Menge von bekannten CNN Modellen mit ausgezeichneten Ergebnissen in internationalen Wettbewerben, derartige wie z.B. die \textit{ImageNet Large Scale Visual Recognition Challenge} (ILSVRC) \cite{russakovskyImageNetLargeScale2015}. Diese Arbeit beschränkt sich auf die Architektur des GoogLeNet Modells und dessen Modifikation PoseNet.

\subsubsection{GoogLeNet}
\label{sec:googlenet}
GoogLeNet, konstruiert von \citet{szegedyGoingDeeperConvolutions2015} im Jahr 2014, ist der Sieger des ILSVRC 2014 Wettbewerbes. Die ILSVRC stellt ungefähr 1000 Bilder aus 1000 unterschiedlichen Kategorien aus dem ImageNet Datensatz bereit. ImageNet ist eine Bildersammlung bestehend aus über 14 Millionen manuell annotierten Bildern in über 20 Tausend unterschiedlichen Kategorien. Die Herausforderung von ILSVRC ist, die 1000 Objekte bestmöglich auf den Bildern zu klassifizieren \cite{russakovskyImageNetLargeScale2015}.

GoogLeNet ist eine besondere Inkarnation des sogenannten \textit{Inception Moduls}, die zeitgleich mit GoogLeNet vorgestellt wird.
Ein Inception Modul beinhaltet mehrere Convolutional- sowie einen Pooling Layer und verarbeitet das Inputvolumen parallel auf 4 Zweigen. Vor rechenintensiven Convolutional Layer werden zusätzliche 1$\times$1 dimensionsreduzierende Layer geschaltet. Abschließend werden die ein Volumen mit der gleichen Breite und Höhe produzierenden Zweige zu einem Outputvolumen (in die Tiefe) zusammengeführt. Ein Inception Modul ermöglicht bei zunehmender Tiefe und Breite den neuronalen Netzen eine konstante Rechenkapazität zu verlangen. Hierzu werden vor den rechenintensiven Operationen eine parallele Berechnung der Zweige und das Einschalten von dimensionsreduzierenden 1$\times$1 Layern benötigt \cite{szegedyGoingDeeperConvolutions2015}. Inzwischen wurde das Inception Modul \cite{szegedyRethinkingInceptionArchitecture2016} und die gesamte Architektur rundherum \cite{szegedyInceptionv4InceptionResNetImpact2016} weiterentwickelt. Abbildung \ref{fig:inception_module} stellt das Inception (v1) Modul dar.

Die Architektur von GoogLeNet besteht aus 9 Inception Modulen (je von Tiefe 2) und hat eine gesamte Tiefe von 22 Layer \footnote{Anzahl der trainierbaren Layern inkl. Output Layer}. Das Ungewöhnliche an der Architektur ist, dass dieser 3 Output-Zweigen hat. Jedes der Ouput-Zweige produziert ein Klassifizierungsergebnis. Output-Zweig 2 \& 3 sind Hilfszweige, die nur in der Trainingsphase einen Einfluss haben und in der Evaluationsphase verworfen werden \cite{szegedyGoingDeeperConvolutions2015}. Abbildung \ref{fig:googlenet} illustriert die GoogLeNet Architektur in Detail.

 \begin{figure}
	\centering
	\includegraphics[width=0.60\textwidth]{images/googlenet/inception_module.png}
	\caption{Darstellung des Inception (v1) Moduls. Das Inputvolumen wird auf 4 Zweigen unabhängig verarbeitet. Die Ergebnisse der unabhängigen Operationen haben die gleiche Breite und Höhe. Das Output des Inception Moduls wird durch das Konkatenieren (in der Tiefe) der einzelnen Ergebnisse bestimmt. Die gelben 1$\times$1 Convolutional Layer dienen zur Dimensionsreduktion. Entnommen aus \cite{szegedyGoingDeeperConvolutions2015}.}
	\label{fig:inception_module}
\end{figure}

\subsubsection{PoseNet}
\label{sec:posenet}
PoseNet, vorgestellt von \citet{kendallPoseNetConvolutionalNetwork2015} im Jahr 2015, basiert auf die im Kapitel \ref{sec:googlenet} eingeführte GoogLeNet Architektur. Statt eine Wahrscheinlichkeitsverteilung der Klassifizierungsergebnisse liefert PoseNet bei Übergabe eines RGB-Bildes die 6 Freiheitsgrade der Kamerapose $y = [\pmb{p};\pmb{q}]$, bestehend aus einem Positionsvektor $\pmb{p} \in \mathbb{R}^{3}$ und einer Quaternion der Orientierung $ \pmb{q} \in \mathbb{R}^{4}$.

PoseNet modifiziert die GoogLeNet Architektur an den Output-Knoten wie folgt (vgl. Abb. \ref{fig:googlenet}) \cite{kendallPoseNetConvolutionalNetwork2015}:
\begin{itemize}
	\item Jedes der \textit{Softmax}-Klassifikatoren werden ersetzt durch Regressoren. Dabei wird der \textit{Softmax-Activation} Layer entfernt und der FC-Layer so modifiziert, dass dieser einen 7-dimensionalen Vektor\footnote{(3) für die Position und (4) für die Orientierung} ausgibt.
	\item Vor dem finalen Regressor des Output-Zweiges 1 wird ein weiteres FC-Layer der Breite 2048 eingefügt.

\end{itemize}
Abbildung \ref{fig:posenet_mods} veranschaulicht die Modifikation von GoogLeNet durch PoseNet.

Die Autoren \citet{kendallPoseNetConvolutionalNetwork2015} stellten zusätzlich eine neue Kostenfunktion mit den Parametern $\hat{y} = [\hat{\pmb{p}};\hat{\pmb{q}}]$ als Soll-Wert und $y = [\pmb{p};\pmb{q}]$ als Ist-Wert vor:
\begin{equation}
	\label{eq:posenet_loss}
	loss(I) = \norm{ \hat{\pmb{p}} - \pmb{p} }_2 + \beta \norm{ \hat{\pmb{q}} - \frac{\pmb{q}}{\norm{\pmb{q}}}}_2
\end{equation}
Der Hyperparameter $\beta$ soll eine Balance der Kosten zwischen dem Positions- und Orientierungsdiskrepanz darstellen. Des Weiteren wird dieser Parameter innerhalb der Gebäuden im Wertebereich von 120 bis 750 und außerhalb des Gebäudes von 250 bis 2000 empfohlen \cite{kendallPoseNetConvolutionalNetwork2015}. 

\begin{figure}[bp]
	\centering
	\includegraphics[width=0.85\textwidth]{images/googlenet/googlenet_diagram.pdf}
	\caption{Der architektonische Aufbau des GoogLeNet Modells. Der Inception-Block wird in Abbildung \ref{fig:inception_module} detaillierter dargestellt. Es gibt 3 Ouput-Zweige, die mit einer identisch aufgebauten Out-Knoten enden. Die Werte der Output-Zweige 2 \& 3 haben nur während der Trainingsphase einen Einfluss und werden in der Evaluationsphase verworfen. In der Trainingsphase wird vor dem Out-Knoten 1 ein Dropout von 40\% angewandt. Jedes Convolutional Layer hat die ReLU (s. Abb. \ref{fig:relu}) als Aktivierungsfunktion. Die \textit{Local-Response-Normalization} \cite{krizhevskyImageNetClassificationDeep2012} ist ein bekanntes Normierungsverfahren und kommt prinzipiell in Verbindung mit der ReLU Aktivierungsfunktion vor. Abbildung basiert auf \cite{szegedyGoingDeeperConvolutions2015}.}
	\label{fig:googlenet}
\end{figure}

 \begin{figure}[bp]
	\centering
	\includegraphics[width=\textwidth]{images/googlenet/posenet_diagram2.pdf}
	\caption{Veranschaulichung der Modifikation von den Output-Zweigen der GoogLeNet Architektur durch PoseNet. Die Softmax Aktivierungsfunktion sowie das fully-connected Layer mit Anzahl der Klassifizierungskategorien als Breite wurde von allen Output-Modulen entfernt. Jeder Output-Zweig hat einen FC-Regressionsschicht erhalten, die die 6 Freiheitsgrade einer Pose bestimmt. Der Output-Zweig 1 wird zusätzlich mit einem weiteren FC-Layer der Breite 2048 vor dem Regressionsschicht erweitert. Die Werte der Output-Zweige 2 \& 3 haben nur während der Trainingsphase einen Einfluss und werden in der Evaluationsphase verworfen. In der Trainingsphase wird vor dem Out-Knoten 1 ein Dropout von 50\% und vor den Out-Knoten 2 \& 3 ein Dropout von 70\% angewandt \cite{kendallPoseNetConvolutionalNetwork2015}.}
	\label{fig:posenet_mods}
\end{figure}