% !TEX root = ../my_thesis.tex

\section{Ergebnisse}
Im vorliegenden Kapitel werden die Ergebnisse der durchgeführten Experimente präsentiert. Zuallererst wurden die Ergebnisse von BIM-PoseNet \cite{acharyaBIMPoseNetIndoorCamera2019} reproduziert und angegeben, um die Korrektheit der Pipeline zu überprüfen. Danach wurde das neuronale Netz für alle synthetische Datensatz mit den Gradientenbilder separat trainiert. Anschließend wurde das neuronale Netz mit den korrespondieren Gradientenbilder der synthetischen sowie realen Daten evaluiert.


\subsection{Reproduktion der Ergebnisse von BIM-PoseNet}
Die Ergebnisse der Experimente von \citet{acharyaBIMPoseNetIndoorCamera2019}, die das PoseNet Model mit Gradientenbilder der karikaturistischen Daten sowie synthetischen Kantenbilder trainieren und anschließend mit den Gradientenbilder der realen Daten evaluieren, konnten näherungsweise (vgl. Tabelle \ref{tab:reproduction}) reproduziert werden. Eine exakte oder bessere Reproduktion der Ergebnisse ist durch Zufall bedingt und wird in dieser Arbeit aus Zeitgründen vernachlässigt.

Abweichend von BIM-PoseNet wurden statt 1000 reale Bilder, 600 reale Bilder evaluiert, weil derzeit 600 Evaluierungsbilder veröffentlicht sind. Die Mengenunterschiede der Evaluierungsdaten sind für die Endergebnisse trivial, da die Akkuratesse sich aus den Durchschnittswerten der Evaluationsergebnisse zusammensetzt und die Daten in zufälliger Reihe evaluiert werden. 

Der Trainingsprozess wurde je Datensatztyp 5-mal wiederholt und die besseren Evaluationsergebnisse wurden behalten. Tabelle \ref{tab:reproduction} präsentiert die Ergebnisse der Reproduktion sowie die Ergebnisse der Autoren \citet{acharyaBIMPoseNetIndoorCamera2019}.


\begin{table}
	\centering
	\caption{Reproduktionsergebnisse. Abweichungen der Ergebnisse sind durch Zufall bedingt und können bei mehrfachem Wiederholen des Trainingsprozesses minimiert bzw. erhoben sowie verbessert werden. }
	\begin{tabularx}{1.0\textwidth}{>{\hsize=1.1\hsize}X >{\hsize=0.95\hsize}X >{\hsize=0.95\hsize}X}
		\textbf{Trainingsdatensatz} \hspace{2cm} (Gradientenbild) & \textbf{BIM-PoseNet} \hspace{2cm} (Position, Orientierung) & \textbf{Reproduktion} \hspace{2cm} (Position, Orientierung)\\
		\hline
	 karikaturistische Simulation & 2.63$m$, 6.99° & 2.57$m$, 10.52°\\
		\hline
		synthetisches Kantenbild & 1.88$m$, 7.73°  & 2.53$m$, 9.54°\\
	\end{tabularx}
	\label{tab:reproduction}
\end{table}


\subsubsection{IC-loop}


\begin{table}[H]
	\centering
	\caption{Übersicht der Hyperparameter.}
	\begin{tabularx}{1.0\textwidth}{X X}
		\textbf{Hyperparameter} & \textbf{Wert}\\
		\hline
		Batchsize & $40$\\
		\hline
		Anzahl der Epochen & $160$\\
		\hline
		$\beta$ der Kostenfunktion (vgl. Gleichung \ref{eq:posenet_loss}) &
		siehe Tabelle \ref{tab:betas} im Abschnitt \ref{subsec:determine_beta}
		\\
		\hline
		Loss-Optimierer & AdaGrad\\
		\hline
		Lernrate & $10^{-3}$\\
		\hline
		Bildskalierung & $480 \times 270$\\
		\hline
		Bildausschnitt& \makecell[tl]{
			$224 \times 244$\\
			(Training: zufällig, Evaluation: zentriert)\\
		}\\
		\hline
		Datensatznormierung & Subtraktion des Durchschnittsbildes der synthetischen Daten \\
		\hline
		Initialisierung der Gewichte & Gewichte eines mit dem Places Datensatz trainierten Models auf GoogLeNet \\
	\end{tabularx}
	\label{tab:betas}
\end{table}

\subsubsection{IC-loop}