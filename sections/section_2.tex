% !TEX root = ../my_thesis.tex
\pagebreak
\section{Grundlagen}



% Lokalisierungsproblem erläutern
% Ansätze aufzählen
% Überführen auf visuelle Lokalisierung

% Arten erläutern
% indirekte methoden
% direkte methoden
% - point-matching
% - rgb-d
% - image only

% Überführen auf DCNNs
\pagebreak
\textbf{Pose Estimation}

\textbf{Deep Convolutional Neural Networks} \textit{(DCNN)} werden erfolgreich im Bereich des Maschinelles Sehens, wie z. B. bei der Klassifizierung von Bildern \cite{krizhevskyImageNetClassificationDeep2012, simonyanVeryDeepConvolutional2014, heDeepResidualLearning2015} sowie bei der  Objekterkennung \cite{girshickRichFeatureHierarchies2013, renFasterRCNNRealTime2015b, girshickFastRCNN2015},  eingesetzt. 
Ein verbreiteter Ansatz beim Entwurf von DCNNs ist das häufig zweckentfremdende Feintunen \textit{(fine-tune)} der Netzwerkarchitekturen, die z. B. für die Bildklassifizierung angesichts der Aufgaben von ImageNet \cite{russakovskyImageNetLargeScale2014} konstruiert wurden. Dieser Ansatz konnte beispielsweise erfolgreich in der Objekterkennung \cite{girshickFastRCNN2015}, Objektsegmentierung \cite{kokkinosPushingBoundariesBoundary2015, maninisConvolutionalOrientedBoundaries2016}, Semantische Segmentierung \cite{nohLearningDeconvolutionNetwork2015, hazirbasFuseNetIncorporatingDepth2017a} und Tiefenbestimmung \cite{liDepthSurfaceNormal2015} verfolgt werden.
Seit Kurzem werden DCNNs auch in den Anwendungsgebieten der Lokalisierung verwendet. Zum Beispiel verwenden Parisotto \etal\cite{parisottoGlobalPoseEstimation2018} DCNNs in Bezug auf das SLAM Problem. Melekhov \etal\cite{melekhovRelativeCameraPose2017} schätzen anhand DCNNs die relative Pose zweier Kameras. Constante \etal\cite{costanteExploringRepresentationLearning2016} und Wang \etal\cite{wangDeepVOEndtoendVisual2017} setzen es im Bereich der visuellen Odometrie ein.

% Posenet überführen & erklären
Geleitet von den \textit{state-of-the-art} Lokalisierungsergebnissen der DCNNs stellen Kendall \etal\cite{kendallPoseNetConvolutionalNetwork2015} den ersten Ansatz zu direkten Posebestimmung  vor. PoseNet \cite{kendallPoseNetConvolutionalNetwork2015} ist die Modifkation der GoogLeNet \cite{szegedyGoingDeeperConvolutions2015} Architektur und zweckentfremdet es von der Bildklassifizierung zu einem Pose-Regressor. Trainiert mit einem Datensatz, bestehend aus Paaren von Farbbild und Pose, kann es die sechs Freiheitsgrade der Kameraposen in unbekannten Szenen mittels eines Bildes bestimmen. Dieser Ansatz benötigt weder Tiefenbilder der Szene noch eine durchsuchbare Bildgalerie. Im Vergleich zu den metrischen Ansätzen wie SLAM oder visuelle Odometrie liefert es eine weniger akkurate Pose. Es bietet jedoch eine hohe Toleranz gegenüber Skalierungs- und Erscheinungsänderungen des Anfragebildes an \cite{piascoSurveyVisualBasedLocalization2018}.

% Varianten aufzählen
Es gibt mehrere Ansätze, die die Genauigkeit von PoseNet übertreffen.
Die Autoren von PoseNet passen ihren Model an einem Bayessian Neural Network \cite{denkerTransformingNeuralNetOutput1991, mackayPracticalBayesianFramework1991} an \cite{kendallModellingUncertaintyDeep2015a}.
Dieselben Autoren erweitern PoseNet mit einer neuen Kostenfunktion unter Berücksichtigung von geometrischen Eigenschaften \cite{kendallGeometricLossFunctions2017}. Wlach \etal\cite{walchImagebasedLocalizationUsing2016} und Clark \etal\cite{clarkVidLocDeepSpatioTemporal2017} setzen Long-Short-Term-Memory \textit{(LSTM)} \cite{hochreiterLongShortTermMemory1997a} Einheiten ein, um Wissen aus der Korrelation von Bildsequenzen zu gewinnen. Wu \etal\cite{wuDelvingDeeperConvolutional2017} und Naseer \etal\cite{naseerDeepRegressionMonocular2017} augmentieren den Trainingsdatensatz. Wu \etal\cite{wuDelvingDeeperConvolutional2017} stocken den vorhandenen Datensatz, indem sie die Bilder künstlich rotieren. Naseer \etal\cite{naseerDeepRegressionMonocular2017} erweitern zuerst über ein weiteres CNN den Datensatz um Tiefenbildern. Anschließend simulieren sie RGB-Bilder aus verschiedenen Viewpoints. Im Vergleich zu PoseNet verwenden Müller \etal\cite{mullerSQUEEZEPOSENETIMAGEBASED2017} und Melekhov \etal\cite{melekhovImageBasedLocalizationUsing2017} eine andere Architektur. 
Das Modell von Müller \etal\cite{mullerSQUEEZEPOSENETIMAGEBASED2017} basiert auf die SqueezeNet \cite{iandolaSqueezeNetAlexNetlevelAccuracy2016} Architektur. Melekhov \etal stellen HourglassNet \cite{melekhovImageBasedLocalizationUsing2017}, basierend auf einen symmetrischen Encoder-Decoder Architektur, vor. Brahmbhatt \etal\cite{brahmbhattGeometryAwareLearningMaps2018} und Valada \etal\cite{valadaDeepAuxiliaryLearning2018, valadaIncorporatingSemanticGeometric} binden zusätzliche Informationen wie z.B. visuelle Odometrie, GPS oder IMU ein. 
Jedes dieser Ansätze benötigen annotierte Traininsdaten. Für die Erstellung solcher Daten wurden beispielsweise mit entsprechender Hardware ausgerüstete Trolleys \cite{huitlTUMindoorExtensiveImage2012}, 3D-Kameras \cite{izadiKinectFusionRealtime3D2011} oder SfM-Methoden \cite{kendallPoseNetConvolutionalNetwork2015} eingesetzt.

% Simulierte 3D-Daten 
\textbf{Simulierte 3D-Daten} werden in der Literatur eingesetzt, um das manuelle Erzeugen und Annotieren von Daten umzugehen. Pishchulin \etal\cite{pishchulinArticulatedPeopleDetection2012a}, Peng \etal\cite{pengLearningDeepObject2014}, Su \etal\cite{suRenderCNNViewpoint2015} und Varol \etal\cite{varolLearningSyntheticHumans2017} erzeugen ihren Trainingsdaten, indem sie virtuelle Objekte auf reale Hintergrundbildern platzieren. Pishchulin \etal\cite{pishchulinArticulatedPeopleDetection2012a} generieren Daten zwecks Personenerkennung und Bestimmung derer körperlicher Pose. Zuvor werden auf den vorhandenen Bildern die körperliche Pose der Personen bestimmt und daran deren 3D Modelle rekonstruiert. Anschließend werden die 3D-Modelle in ihrer Pose variiert auf reale Hintergrundbildern platziert. Peng \etal\cite{pengLearningDeepObject2014} erstellen Daten um Objekte auf realen Bildern zu detektieren. Von jeder Objektklasse werden 3D-Modelle auf einem Hintergrundbild aus einer Sammlung gelegt. Su \etal\cite{suRenderCNNViewpoint2015} generieren einen großen Datensatz mit 3D-Modellen, um den Viewpoint von Objekten auf realen Bildern zu bestimmen. Bei dieser Datengenerierung wird jedes virtuelle Objekt auf zufällige Hintergrundbildern positioniert und mit unterschiedlichen Konfigurationen \textit{(wie z.B. Beleuchtung)} gerendert. 
Varol \etal\cite{varolLearningSyntheticHumans2017} erstellen künstliche Personen auf Bildern, um beispielsweise den menschlichen Körper in seine Glieder zu segmentieren. Dabei rendern sie zufälligen virtuellen Person mit zufälliger Pose auf beliebige Hintergrundbildern.
Fanello \etal\cite{fanelloLearningBeDepth2014} rendert künstliche Infrarotbilder von Händen und Gesichtern zwecks Tiefenerkennung und Segmentierung der Körperteile aus einem RGB-Bild.
Dosovitskiy \etal\cite{dosovitskiyFlowNetLearningOptical2015} erlernen mit synthetischen Daten den optischen Fluss von Bildsequenzen.  Hierbei werden auf Hintergrundbildern aus einer Sammlung mehrmals bewegte virtuelle Stühle platziert.

Motiviert von der Datengenerierung über 3D-simulierten Daten stellt Ha \etal\cite{haImagebasedIndoorLocalization2018} einen Ansatz zur Bild-basierte Lokalisierung in Gebäuden vor. Dieser Forschungsansatz generiert synthetische Daten aus einem Building Information Modeling \textit{(BIM)}. Hierbei werden die durch ein das VGG Netzwerk \cite{simonyanVeryDeepConvolutional2014} extrahierte Features als wesentlich erachtet und in einer Datenbank gepflegt. Ein reales Aufnahmebild im Gebäude lässt sich durch den Vergleich der Features
lokalisieren. 

% bim-indoor und modelling
% 
