% !TEX root = ../my_thesis.tex
\pagebreak
\section{Grundlagen}



% Lokalisierungsproblem erläutern
% Ansätze aufzählen
% Überführen auf visuelle Lokalisierung

% Arten erläutern
% indirekte methoden
% direkte methoden
% - point-matching
% - rgb-d
% - image only
% Überführen auf CNNs
\pagebreak
\textbf{Deep Convolutional Neural Networks} \textit{(DCNN)} werden erfolgreich im Bereich des Maschinelles Sehen, wie z. B. bei der Klassifizierung von Bildern \cite{krizhevskyImageNetClassificationDeep2012, simonyanVeryDeepConvolutional2014, heDeepResidualLearning2015} sowie bei der  Objekterkennung \cite{girshickRichFeatureHierarchies2013, renFasterRCNNRealTime2015b, girshickFastRCNN2015},  eingesetzt. 
Ein verbreiteter Ansatz beim Entwurf von DCNNs ist das häufig zweckentfremdende Feinabstimmung \textit{(fine-tune)} der Netzwerkarchitekturen, die z. B. für die Bildklassifizierung angesichts der Aufgaben von ImageNet \cite{russakovskyImageNetLargeScale2014} konstruiert wurden. Dieser Ansatz konnte beispielsweise erfolgreich in der Objekterkennung \cite{girshickFastRCNN2015}, Objektsegmentierung \cite{kokkinosPushingBoundariesBoundary2015, maninisConvolutionalOrientedBoundaries2016}, Semantische Segmentierung \cite{nohLearningDeconvolutionNetwork2015, hazirbasFuseNetIncorporatingDepth2017a} und Tiefenbestimmung \cite{liDepthSurfaceNormal2015} verfolgt werden.
Seit Kurzem werden DCNNs auch in den Anwendungsgebieten der Lokalisierung verwendet. Zum Beispiel verwenden Parisotto et al. DCNNs in Bezug auf das SLAM Problem \cite{parisottoGlobalPoseEstimation2018}. Melekhov et al. schätzen anhand DCNNs die relative Pose zweier Kameras \cite{melekhovRelativeCameraPose2017}. Constante et al. und Wang et el. setzen es im Bereich der visuellen Odometrie ein \cite{costanteExploringRepresentationLearning2016, wangDeepVOEndtoendVisual2017}.

% Posenet überführen
Geleitet von den \textit{state-of-the-art} Lokalisierungsergebnissen der DCNNs stellen Kendall et al. PoseNet \cite{kendallPoseNetConvolutionalNetwork2015} vor.
% Posenet erklären
PoseNet ist die Feinabstimmung der GoogLeNet \cite{szegedyGoingDeeperConvolutions2015} Architektur und zweckentfremdet es von der Bildklassifizierung zu einem Pose-Regressor. Trainiert mit einem geringeren Datensatz bestehend aus Paaren von Farbbild und Pose, kann es die sechs Freiheitsgrade der Kameraposen in unbekannten Szenen bestimmen. Dieser Ansatz benötigt weder Tiefenbilder der Szene noch eine durchsuchbare Bildgalerie. Im Vergleich zu den metrischen Ansätzen wie SLAM oder visuelle Odometrie liefert es eine weniger akkurate Pose, jedoch bietet es eine hohe Toleranz gegenüber Skalierungs- und Erscheinungsänderungen des Anfragebildes an \cite{piascoSurveyVisualBasedLocalization2018}.


% Varianten aufzählen
Für die Verbesserung der Ergebnisse von PoseNet wurden viele Ansätze vorgestellt. Eine Genauigkeitzunahmen erhalten die Autoren durch von PoseNet stellen die Autoren selbst, indem sie Ungewisse mit. Weiterhin können die Autoren mit einer neuen Kostenfunktion, unter Berücksichtigung von geometrischen Eigenschaften, die Genauigkeit der Ergebnisse verbessern \cite{kendallGeometricLossFunctions2017}.

Wlach et al. \cite{walchImagebasedLocalizationUsing2016} und Clark et al. \cite{clarkVidLocDeepSpatioTemporal2017} verwenden GoogLeNet, um Merkmale von sukzessiven Bilder zu extrahieren. Die Pose wird bestimmt durch die Übergabe der Merkmale an Long-Short-Term-Memory (LSTM) \cite{hochreiterLongShortTermMemory1997a} Einheiten.
Wu et al. augmentieren den Trainingsdatensatz mit rotierten Bildern \cite{wuDelvingDeeperConvolutional2017}.
Im Vergleich zu PoseNet verwenden Müller et al. \cite{mullerSQUEEZEPOSENETIMAGEBASED2017} und Melekhov et al.\cite{melekhovImageBasedLocalizationUsing2017} eine andere Architektur.

Die oben aufgeführten Ansätze sind abhängig von SfM methotds.

% Simulierte Daten / syntethic

% Machine learning + syntethic

% 
