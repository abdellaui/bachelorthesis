% !TEX root = ../my_thesis.tex
\pagebreak
\section{Grundlagen}



% Lokalisierungsproblem erläutern
% Ansätze aufzählen
% Überführen auf visuelle Lokalisierung

% Arten erläutern
% indirekte methoden
% direkte methoden
% - point-matching
% - rgb-d
% - image only
% Überführen auf CNNs
\pagebreak
\textbf{Deep Convolutional Neural Networks} \textit{(DCNN)} werden erfolgreich im Bereich des Maschinelles Sehen, wie z. B. bei der Klassifizierung von Bildern \cite{krizhevskyImageNetClassificationDeep2012, simonyanVeryDeepConvolutional2014, heDeepResidualLearning2015} sowie bei der  Objekterkennung \cite{girshickRichFeatureHierarchies2013, renFasterRCNNRealTime2015b, girshickFastRCNN2015},  eingesetzt. 
Ein verbreiteter Ansatz beim Entwurf von DCNNs ist das häufig zweckentfremdende Feintunen \textit{(fine-tune)} der Netzwerkarchitekturen, die z. B. für die Bildklassifizierung angesichts der Aufgaben von ImageNet \cite{russakovskyImageNetLargeScale2014} konstruiert wurden. Dieser Ansatz konnte beispielsweise erfolgreich in der Objekterkennung \cite{girshickFastRCNN2015}, Objektsegmentierung \cite{kokkinosPushingBoundariesBoundary2015, maninisConvolutionalOrientedBoundaries2016}, Semantische Segmentierung \cite{nohLearningDeconvolutionNetwork2015, hazirbasFuseNetIncorporatingDepth2017a} und Tiefenbestimmung \cite{liDepthSurfaceNormal2015} verfolgt werden.
Seit Kurzem werden DCNNs auch in den Anwendungsgebieten der Lokalisierung verwendet. Zum Beispiel verwenden Parisotto et al. \cite{parisottoGlobalPoseEstimation2018} DCNNs in Bezug auf das SLAM Problem. Melekhov et al. \cite{melekhovRelativeCameraPose2017} schätzen anhand DCNNs die relative Pose zweier Kameras. Constante et al.  \cite{costanteExploringRepresentationLearning2016} und Wang et el \cite{wangDeepVOEndtoendVisual2017} setzen es im Bereich der visuellen Odometrie ein.

% Posenet überführen
Geleitet von den \textit{state-of-the-art} Lokalisierungsergebnissen der DCNNs stellen Kendall et al. PoseNet \cite{kendallPoseNetConvolutionalNetwork2015} vor.
% Posenet erklären
PoseNet ist das Feintuning der GoogLeNet \cite{szegedyGoingDeeperConvolutions2015} Architektur und zweckentfremdet es von der Bildklassifizierung zu einem Pose-Regressor. Trainiert mit einem geringeren Datensatz, bestehend aus Paaren von Farbbild und Pose, kann es die sechs Freiheitsgrade der Kameraposen in unbekannten Szenen bestimmen. Dieser Ansatz benötigt weder Tiefenbilder der Szene noch eine durchsuchbare Bildgalerie. Im Vergleich zu den metrischen Ansätzen wie SLAM oder visuelle Odometrie liefert es eine weniger akkurate Pose, jedoch bietet es eine hohe Toleranz gegenüber Skalierungs- und Erscheinungsänderungen des Anfragebildes an \cite{piascoSurveyVisualBasedLocalization2018}.


% Varianten aufzählen
Es gibt mehrere Ansätze, die die Genauigkeit von PoseNet übertreffen .
% Modelling uncertainy
Dieselben Autoren erweitern PoseNet mit einer, unter Berücksichtigung von geometrischen Eigenschaften, neue Kostenfunktion \cite{kendallGeometricLossFunctions2017}. Wlach et al. \cite{walchImagebasedLocalizationUsing2016} und Clark et al. \cite{clarkVidLocDeepSpatioTemporal2017} verwenden GoogLeNet, um Merkmale von sukzessiven Bilder zu extrahieren. Die Pose wird bestimmt durch die Übergabe der Merkmale an Long-Short-Term-Memory (LSTM) \cite{hochreiterLongShortTermMemory1997a} Einheiten.
Wu et al.\cite{wuDelvingDeeperConvolutional2017}  und  Naseer et al. \cite{naseerDeepRegressionMonocular2017} augmentieren den Trainingsdatensatz. Im Vergleich zu PoseNet verwenden Müller et al. \cite{mullerSQUEEZEPOSENETIMAGEBASED2017} und Melekhov et al.\cite{melekhovImageBasedLocalizationUsing2017} eine andere Architektur. Brahmbhatt et al. \cite{brahmbhattGeometryAwareLearningMaps2018} und Valada et al. \cite{valadaDeepAuxiliaryLearning2018, valadaIncorporatingSemanticGeometric} binden zusätzliche Informationen wie z.B. visuelle Odometrie, GPS oder IMU ein.
Die oben aufgeführten Ansätze sind abhängig von SfM methotds.

% Simulierte Daten / syntethic

% Machine learning + syntethic

% 
