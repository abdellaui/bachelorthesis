% !TEX root = ../my_thesis.tex

\section{Stand der Forschung und Grundlagen}
\label{sec:kapitel_2}
Das vorliegende Kapitel versucht einen Überblick des Forschungsstandes in den unterschiedlichen Aspekten der Arbeit zu verschaffen. Anschließend werden notwendige Grundkenntnisse vermittelt.

\subsection{Hintergrund}

\textbf{Pose Estimation} wird in dieser Arbeit als eine Methode der \textit{visuellen Lokalisierung} (Visual-Based Localization, kurz VBL) betrachtet. VBL beschäftigt sich mit der Bestimmung der globalen Pose eines visuellen Abfragematerials (z.B. ein RGB-Bild) in einer zuvor bekannten Szene \cite{piascoSurveyVisualBasedLocalization2018}.
Ein naheliegendes Themengebiet der Robotik ist die \textit{visuelle Ortswiedererkennung} (Visual Place Recognition, kurz VPR) \cite{lowryVisualPlaceRecognition2016}. VPR fokussiert sich auf das Feststellen eines bereits besuchten Ortes und definiert sich aus einem Mapping-, einem Datenverarbeitungs- und einem Orientierungsmodul. Allgemein lässt sich der Prozess eines VPRs wie folgt beschreiben: Eine interne Karte bekannter Orte wird durch das Mappingmodul verwaltet. Die Daten werden vom Datenverarbeitungsmodul vorbereitet und anschließend an das Orientierungsmodul übergeben. Daraufhin bestimmt das Orientierungsmodul die Pose und entscheidet mit der immer aktuell gehaltenen Karte, ob ein Ort bereits besucht wurde. Im Vergleich zur VPR versucht die visuelle Lokalisierung eine Pose zu bestimmen und benötigt daher neben den zwei Modulen kein Mappingmodul \cite{lowryVisualPlaceRecognition2016}.

% hat 2 katogiren..
Die rein visuellen Methoden des VBLs unterteilen sich in direkte und indirekte Methoden \cite{lowryVisualPlaceRecognition2016}. Die indirekten Methoden behandeln das Lokalisierungsproblem als eine Bildersuche in einer Datenbank, ähnlich wie das \textit{Content-Based-Image-Retrieval} \cite{lewContentbasedMultimediaInformation2006} Problem. Dabei wird das Abfragebild über eine Ähnlichkeitsfunktion mit den Vergleichsbildern aus der Datenbank abgeglichen \cite{zhangImageBasedLocalization2006}. Diese Art von Methoden benötigen eine speicherintensive Bildergalerie (Datenbank) und liefern Ergebnisse bei einem korrespondierenden Bildfund \cite{lowryVisualPlaceRecognition2016}. Hingegen versuchen die direkten Methoden die Pose über eine Referenzumgebung zu bestimmen und benötigen daher meist keine große Datenbank \cite{piascoSurveyVisualBasedLocalization2018}. Es gibt drei Arten der direkten Methoden: 
\begin{enumerate*}[label=\arabic*)]
	\item Matching von Features zu Punktwolken (z.B. \cite{liWorldwidePoseEstimation2012}),
	\item Pose Regression mit Tiefenbildern (z.B. \cite{shottonSceneCoordinateRegression2013}) und
	\item Pose Regression nur mit Bildern (z.B. \cite{kendallPoseNetConvolutionalNetwork2015}).
\end{enumerate*}

Die erste Art der direkten Methoden versucht die Pose zu bestimmen, indem die 2D-3D-Korrespondenz direkt über eine repräsentative 3D-Punktwolke der Szene hergestellt wird \cite{piascoSurveyVisualBasedLocalization2018}. Diese unterscheidet sich von den indirekten Methoden durch die aktive Zuordnung der 2D-Features in die 3D-Punktwolke, statt das passive Vergleichen der Features in einer Datenbank \cite{irscharaStructurefrommotionPointClouds2009}. Die zweite Art der direkten Methoden bestimmt anhand von Tiefenbildern die Pose z.B. über Regression Forests \cite{shottonSceneCoordinateRegression2013}, Randomize Ferns \cite{glockerRealTimeRGBDCamera2015}, Coarse-to-Fine Registrierung \cite{santosMappingIndoorSpaces2016} oder künstlichen neuronalen Netze \cite{massicetiRandomForestsNeural2017}. Diese Forschungsprojekte liefern mit den Tiefenbildern die gewünschten Resultate. Allerdings sind hierfür notwendige 3D-Kameras nicht verbreitet. Die dritte Art der direkten Methoden bestimmt die Pose nur mit den RGB-Bildern \cite{kendallPoseNetConvolutionalNetwork2015}.

\textbf{Convolutional Neural Networks} (CNN) werden erfolgreich im Bereich des Maschinellen Sehens eingesetzt, wie z.B. bei der Klassifizierung von Bildern \cite{krizhevskyImageNetClassificationDeep2012} sowie bei der Objekterkennung \cite{girshickRichFeatureHierarchies2014}. 
Ein verbreiteter Ansatz beim Entwurf von CNNs ist das Modifizieren der vorhandenen Netzwerkarchitekturen, die z.B. für die Bildklassifizierung angesichts der Wettbewerbe von \textit{ImageNet Large Scale Visual Recognition Challenge} (ILSVRC) \cite{russakovskyImageNetLargeScale2015} konstruiert wurden. Dieser Ansatz konnte beispielsweise erfolgreich in der Objekterkennung \cite{girshickFastRCNN2015a}, Objektsegmentierung \cite{kokkinosPushingBoundariesBoundary2015}, semantischen Segmentierung \cite{ hazirbasFuseNetIncorporatingDepth2017} und Tiefenbestimmung \cite{boliDepthSurfaceNormal2015} verfolgt werden. CNNs werden auch in den Anwendungsgebieten der Lokalisierung verwendet. Zum Beispiel verwendeten \citet{parisottoGlobalPoseEstimation2018} die CNNs in Bezug auf das SLAM Problem. \citet{melekhovRelativeCameraPose2017} schätzten anhand der CNNs die relative Pose zweier Kameras. \citet{costanteExploringRepresentationLearning2016} und \citet{wangDeepVOEndtoendVisual2017} setzten diese im Bereich der VO ein.

% Posenet überführen & erklären
Geleitet von den \textit{state-of-the-art} Lokalisierungsergebnissen der CNNs stellten \citet{kendallPoseNetConvolutionalNetwork2015} den ersten Ansatz zur direkten Posebestimmung über die CNNs nur mit den RGB-Bildern vor. \textit{PoseNet} ist die Modifikation der \textit{GoogLeNet}-Architektur \cite{szegedyGoingDeeperConvolutions2015} und zweckentfremdet diese von der Bildklassifizierung zu einem Pose-Regressor. Trainiert mit einem Datensatz, bestehend aus einem Paar von einem Farbbild und einer Pose, kann es die sechs Freiheitsgrade der Kamerapose in unbekannten Szenen mittels eines Bildes bestimmen. Dieser Ansatz benötigt keine durchsuchbare Bildgalerie, weder eine Punktwolke noch ein Tiefenbild der Szene. Im Vergleich zu den Ansätzen wie SLAM oder VO bestimmt der Ansatz mit PoseNet eine weniger akkurate Pose. Allerdings weist PoseNet eine bessere Toleranz gegenüber Skalierungs- und Erscheinungsänderungen des Anfragebildes auf \cite{piascoSurveyVisualBasedLocalization2018}.

% Varianten aufzählen
Es gibt mehrere Ansätze, die die Genauigkeit von PoseNet übertreffen.
Einen Fortschritt erhielten die Autoren von PoseNet durch die hier \cite{kendallModellingUncertaintyDeep2016} vorgestellte Anpassung ihres Modells an einen \textit{Bayessian Neural Network} \cite{denkerTransformingNeuralNetOutput1991}.
Dieselben Autoren erweiterten PoseNet mit einer neuen Kostenfunktion unter Berücksichtigung von geometrischen Eigenschaften \cite{kendallGeometricLossFunctions2017}. \citet{walchImageBasedLocalizationUsing2017} und \citet{clarkVidLocDeepSpatioTemporal2017} setzten die \textit{Long-Short-Term-Memory} (LSTM) \cite{hochreiterLongShortTermMemory1997} Einheiten ein, um eine Wissenserweiterung aus der Korrelation von Bildsequenzen zu erlangen. \citet{wuDelvingDeeperConvolutional2017} wie ebenfalss \citet{naseerDeepRegressionMonocular2017} augmentierten den Trainingsdatensatz. \citet{wuDelvingDeeperConvolutional2017} vermehrten den vorhandenen Datensatz, indem sie die Bilder künstlich rotierten. \citet{naseerDeepRegressionMonocular2017} erweiterten zuerst über einen weiteren CNN den Datensatz um weitere Tiefenbilder. Anschließend simulierten die Autoren RGB-Bilder aus verschiedenen \textit{Viewpoints}. Im Vergleich zu PoseNet verwendeten \citet{mullerSQUEEZEPOSENETIMAGEBASED2017} und \citet{melekhovImageBasedLocalizationUsing2017} eine andere Architektur. 
Das Modell von \citet{mullerSQUEEZEPOSENETIMAGEBASED2017} basierte auf die \textit{SqueezeNet} \cite{iandolaSqueezeNetAlexNetlevelAccuracy2016} Architektur. \citet{melekhovImageBasedLocalizationUsing2017a} stellten \textit{HourglassNet} vor, das auf einem symmetrischen \textit{Encoder-Decoder}-Architektur basiert. \citet{brahmbhattGeometryAwareLearningMaps2018} und \citet{valadaDeepAuxiliaryLearning2018, valadaIncorporatingSemanticGeometric2018} banden zusätzliche Informationen wie z.B. VO, GPS oder \textit{Inertial-Measurement-Units} (IMU) ein. 

Jedes dieser Ansätze benötigen annotierte Trainingsdaten. Für die Erstellung solcher Daten wurden beispielsweise mit einer entsprechenden Hardware ausgerüsteten Trolleys \cite{huitlTUMindoorExtensiveImage2012}, 3D-Kameras mit \textit{Iterative-Closest-Point} Algorithmen \cite{izadiKinectFusionRealtime3D2011} oder SfM-Methoden \cite{kendallPoseNetConvolutionalNetwork2015} eingesetzt.
\\
% Simulierte 3D-Daten 
\textbf{Simulierte 3D-Daten} werden in der Literatur oft eingesetzt, um das manuelle Erzeugen und Annotieren von Daten umzugehen. \citet{pishchulinArticulatedPeopleDetection2012a}, \citet{pengLearningDeepObject2015}, \citet{suRenderCNNViewpoint2015} und \citet{varolLearningSyntheticHumans2017} erzeugten ihre Trainingsdaten, indem sie virtuelle Objekte auf reale Hintergrundbildern platzierten. \citet{pishchulinArticulatedPeopleDetection2012a} generierten die Daten zwecks der Personenerkennung und der Bestimmung derer körperlichen Pose. Zuvor wurden auf den vorhandenen Bildern die körperliche Pose der Personen bestimmt und daran deren 3D-Modelle rekonstruiert. Anschließend wurden die in ihrer Pose variierten 3D-Modelle auf realen Hintergrundbildern platziert. Die Autoren konnten vergleichbare Ergebnisse zu den vorhandenen Ansätzen mit realen Daten ermitteln. \citet{pengLearningDeepObject2015} erstellten die Daten, um Objekte auf realen Bildern zu detektieren. Von jeder Objektklasse wurden 3D-Modelle auf einem Hintergrundbild aus einer Sammlung gelegt. Nach den Feststellungen der Autoren führte das Feintunen eines Netzwerks mit synthetischen Daten zu einer Abnahme der Akkuratesse, wenn das Netzwerk vorher nur für die Detektion eines Objektes trainiert wurde. Hingegen konnten \citet{pengLearningDeepObject2015} eine Steigung der Ergebnisse beim Feintunen mit simulierten Daten auf einem Netzwerk ermitteln, das auf die Detektion von mehreren Objekten trainiert wurde. \citet{suRenderCNNViewpoint2015} generierten einen großen Datensatz mit 3D-Modellen, um den \textit{Viewpoint} von Objekten auf realen Bildern zu bestimmen. Bei dieser Datengenerierung wurde jedes virtuelle Objekt auf einem zufälligen Hintergrundbild positioniert und mit unterschiedlichen Konfigurationen (z.B. Beleuchtung) gerendert. Die Autoren konnten mit der Datenaugmentierung \textit{state-of-the-art Viewport-Estimation}-Methoden der \textit{PASCAL 3D+}\cite{xiangPASCALBenchmark3D2014} \textit{Benchmark} übertreffen \cite{suRenderCNNViewpoint2015}. \citet{varolLearningSyntheticHumans2017} erstellten auf Bildern künstliche Personen, um beispielsweise den menschlichen Körper in seine Glieder zu segmentieren. Dabei renderten die Autoren zufällige virtuelle Personen mit zufälliger Pose auf beliebige Hintergrundbilder und konnten durch das Trainieren mit den erzeugten Daten eine Steigung der Akkuratesse einiger CNNs zeigen. \citet{fanelloLearningBeDepth2014} renderten künstliche Infrarotbilder von Händen und Gesichtern mit dem Ziel einer Tiefenerkennung wie auch einer Segmentierung der Hand in einzelne Finger und des Gesichtes in Bereiche aus einem RGB-Bild. Die Autoren konnten konventionelle Methoden über Helligkeitsabfall übertreffen und vergleichbare Ergebnisse zu den Ansätzen mit einer herkömmlichen 3D-Kamera erzielen. \citet{dosovitskiyFlowNetLearningOptical2015} erlernten mit synthetischen Daten den optischen Fluss von Bildsequenzen. Hierbei wurden auf den Hintergrundbildern aus einer Sammlung mehrmals bewegte virtuelle Stühle platziert. Die Autoren konnten die \textit{state-of-the-art} Ansätze mit reale Daten mit synthetischen Daten übertreffen \cite{dosovitskiyFlowNetLearningOptical2015}.

Motiviert von der Datengenerierung über 3D-simulierten Daten stellten \citet{haImagebasedIndoorLocalization2018} einen Ansatz zur visuellen Lokalisierung in Gebäuden vor. Dieser Forschungsansatz generierte synthetische Daten aus einem \textit{Building-Information-Modeling} (BIM). Bei den Daten wurden die durch das vortrainierte \textit{VGG Netzwerk} \cite{simonyanVeryDeepConvolutional2014} extrahierte Features als wesentlich erachtet und in einer Datenbank gepflegt. Ein reales Aufnahmebild im Gebäude ließ sich durch den Vergleich der Features lokalisieren. \citet{acharyaBIMPoseNetIndoorCamera2019} erzeugten ebenso Trainingsdaten aus einem BIM. Im Gegensatz zu \citet{haImagebasedIndoorLocalization2018} verwendeten \citet{acharyaBIMPoseNetIndoorCamera2019} zur Lokalisierung keine Datenbank bedürftiges Verfahren, sondern bestimmten die Pose direkt über PoseNet. Die Daten wurden entlang einer ca. 18$m$ langen Strecke in der Simulation eines ca. $18m \times 4m$ großen Korridors gesammelt. Hierbei wurden synthetische Daten erzeugt, die sich in der Realitätstreue vom Karikaturistischen zum Fotorealistisch-texturierten unterschieden. Trainiert mit den unterschiedlichen synthetischen Daten und evaluiert mit den realen Daten erzielen die Forscher eine Akkuratesse von ca. $5m$ in der Position und 20° in der Orientierung.
Die besten Ergebnisse erzielten die Autoren durch das Training mit den Gradientenbildern der karikaturistischen Daten und synthetischen Kantenbildern sowie die Evaluation mit den Gradientenbildern der realen Aufnahmen. Die Autoren erhielten hierbei eine Akkuratesse von ca. $2m$, 7° \cite{acharyaBIMPoseNetIndoorCamera2019}.

Die vorliegende Arbeit versucht den Ansatz von \citet{acharyaBIMPoseNetIndoorCamera2019} in größeren Gebäudesimulationen und auf längeren Strecken zu untersuchen, worin das PoseNet Modell mit den Gradientenbildern der synthetischen Daten trainiert und mit den Gradientenbildern der realen Daten evaluiert wird.

Im weiteren Verlauf des Kapitels werden einige grundlegende Themen erläutert. Zuerst werden künstliche neuronale Netze definiert. Danach wird ein elementares Wissen an Convolutional Neural Networks vermittelt und anschließend bekannte CNN Modelle näher erläutert.

\input{sections/subsections_2.tex}