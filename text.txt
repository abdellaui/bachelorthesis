





Pose Estimation in Gebäuden anhand von simulierten 3D Daten und Convolutional Neural Networks
Abdullah Sahin
Bachelor
Angewandte Informatik
April 4th, 2019
108016202304
Prof. Dr.-Ing. Markus König
Patrick Herbers, M.Sc.

.....


	


Einleitung

Fügen Sie hier Ihren Text ein. Klicken Sie nach der Prüfung auf die farbig unterlegten Textstellen. oder nutzen Sie diesen Text als Beispiel für ein Paar Fehler , die LanguageTool erkennen kann: Ihm wurde Angst und bange. Mögliche stilistische Probleme werden blau hervorgehoben: Das ist besser wie vor drei Jahren. Eine Rechtschreibprüfun findet findet übrigens auch statt. Donnerstag, den 23.01.2019 war gutes Wetter. Die Beispiel endet hier.


Stand der Forschung und Grundlagen















Pose Estimation

Deep Convolutional Neural Networks (DCNN) werden erfolgreich im Bereich des Maschinelles Sehens, wie z. B. bei der Klassifizierung von Bildern sowie bei der  Objekterkennung,  eingesetzt. 
Ein verbreiteter Ansatz beim Entwurf von DCNNs ist das häufig zweckentfremdende Feintunen (fine-tune) der Netzwerkarchitekturen, die z. B. für die Bildklassifizierung angesichts der Aufgaben von ImageNet konstruiert wurden. Dieser Ansatz konnte beispielsweise erfolgreich in der Objekterkennung, Objektsegmentierung, Semantische Segmentierung  und Tiefenbestimmung verfolgt werden.
Seit Kurzem werden DCNNs auch in den Anwendungsgebieten der Lokalisierung verwendet. Zum Beispiel verwenden Parisotto parisottoGlobalPoseEstimation2018 DCNNs in Bezug auf das SLAM Problem. Melekhov melekhovRelativeCameraPose2017 schätzen anhand DCNNs die relative Pose zweier Kameras. Constante costanteExploringRepresentationLearning2016 und Wang wangDeepVOEndtoendVisual2017 setzen es im Bereich der visuellen Odometrie ein.


Geleitet von den state-of-the-art Lokalisierungsergebnissen der DCNNs stellen Kendall kendallPoseNetConvolutionalNetwork2015 den ersten Ansatz zu direkten Posebestimmung  vor. PoseNet ist die Modifkation der GoogLeNet Architektur und zweckentfremdet es von der Bildklassifizierung zu einem Pose-Regressor. Trainiert mit einem Datensatz, bestehend aus Paaren von Farbbild und Pose, kann es die sechs Freiheitsgrade der Kameraposen in unbekannten Szenen mittels eines Bildes bestimmen. Dieser Ansatz benötigt weder Tiefenbilder der Szene noch eine durchsuchbare Bildgalerie. Im Vergleich zu den metrischen Ansätzen wie SLAM oder visuelle Odometrie liefert es eine weniger akkurate Pose. Es bietet jedoch eine hohe Toleranz gegenüber Skalierungs- und Erscheinungsänderungen des Anfragebildes an.


Es gibt mehrere Ansätze, die die Genauigkeit von PoseNet übertreffen.
Einen Fortschritt erhalten die Autoren von PoseNet durch die hier vorgestellte Anpassung ihres Models an einem Bayessian Neural Network.
Dieselben Autoren erweitern PoseNet mit einer neuen Kostenfunktion unter Berücksichtigung von geometrischen Eigenschaften. Wlach walchImagebasedLocalizationUsing2016 und Clark clarkVidLocDeepSpatioTemporal2017 setzen Long-Short-Term-Memory (LSTM) Einheiten ein, um Wissen aus der Korrelation von Bildsequenzen zu gewinnen. Wu wuDelvingDeeperConvolutional2017 und Naseer naseerDeepRegressionMonocular2017 augmentieren den Trainingsdatensatz. Wu wuDelvingDeeperConvolutional2017 stocken den vorhandenen Datensatz auf, indem sie die Bilder künstlich rotieren. Naseer naseerDeepRegressionMonocular2017 erweitern zuerst über ein weiteres CNN den Datensatz um Tiefenbildern. Anschließend simulieren sie RGB-Bilder aus verschiedenen Viewpoints. Im Vergleich zu PoseNet verwenden Müller mullerSQUEEZEPOSENETIMAGEBASED2017 und Melekhov melekhovImageBasedLocalizationUsing2017 eine andere Architektur. 
Das Modell von Müller mullerSQUEEZEPOSENETIMAGEBASED2017 basiert auf die SqueezeNet Architektur. Melekhov stellen HourglassNet, basierend auf einen symmetrischen Encoder-Decoder Architektur, vor. Brahmbhatt brahmbhattGeometryAwareLearningMaps2018 und Valada valadaDeepAuxiliaryLearning2018, valadaIncorporatingSemanticGeometric binden zusätzliche Informationen wie z.B. visuelle Odometrie, GPS oder IMU ein. 
Jedes dieser Ansätze benötigen annotierte Traininsdaten. Für die Erstellung solcher Daten wurden beispielsweise mit entsprechender Hardware ausgerüstete Trolleys, 3D-Kameras oder SfM-Methoden eingesetzt.


Simulierte 3D-Daten werden in der Literatur eingesetzt, um das manuelle Erzeugen und Annotieren von Daten umzugehen. Pishchulin pishchulinArticulatedPeopleDetection2012a, Peng pengLearningDeepObject2014, Su suRenderCNNViewpoint2015 und Varol varolLearningSyntheticHumans2017 erzeugen ihren Trainingsdaten, indem sie virtuelle Objekte auf reale Hintergrundbildern platzieren. Pishchulin pishchulinArticulatedPeopleDetection2012a generieren Daten zwecks Personenerkennung und Bestimmung derer körperlicher Pose. Zuvor werden auf den vorhandenen Bildern die körperliche Pose der Personen bestimmt und daran deren 3D Modelle rekonstruiert. Anschließend werden die 3D-Modelle in ihrer Pose variiert auf reale Hintergrundbildern platziert. Peng pengLearningDeepObject2014 erstellen Daten um Objekte auf realen Bildern zu detektieren. Von jeder Objektklasse werden 3D-Modelle auf einem Hintergrundbild aus einer Sammlung gelegt. Su suRenderCNNViewpoint2015 generieren einen großen Datensatz mit 3D-Modellen, um den Viewpoint von Objekten auf realen Bildern zu bestimmen. Bei dieser Datengenerierung wird jedes virtuelle Objekt auf zufällige Hintergrundbildern positioniert und mit unterschiedlichen Konfigurationen (wie z.B. Beleuchtung) gerendert. 
Varol varolLearningSyntheticHumans2017 erstellen künstliche Personen auf Bildern, um beispielsweise den menschlichen Körper in seine Glieder zu segmentieren. Dabei rendern sie zufälligen virtuellen Person mit zufälliger Pose auf beliebige Hintergrundbildern.
Fanello fanelloLearningBeDepth2014 rendert künstliche Infrarotbilder von Händen und Gesichtern zwecks Tiefenerkennung und Segmentierung der Körperteile aus einem RGB-Bild.
Dosovitskiy dosovitskiyFlowNetLearningOptical2015 erlernen mit synthetischen Daten den optischen Fluss von Bildsequenzen.  Hierbei werden auf Hintergrundbildern aus einer Sammlung mehrmals bewegte virtuelle Stühle platziert.

Motiviert von der Datengenerierung über 3D-simulierten Daten stellt Ha haImagebasedIndoorLocalization2018 einen Ansatz zur Bild-basierte Lokalisierung in Gebäuden vor. Dieser Forschungsansatz generiert synthetische Daten aus einem Building Information Modeling (BIM). Hierbei werden die durch das vortrainierte VGG Netzwerk extrahierte Features als wesentlich erachtet und in einer Datenbank gepflegt. Ein reales Aufnahmebild im Gebäude lässt sich durch den Vergleich der Features lokalisieren. 

Acharya acharyaBIMPoseNetIndoorCamera2019, acharyaMODELLINGUNCERTAINTYSINGLE2019 erzeugen ebenso Trainingsdaten aus einem BIM, jedoch verwenden sie zur Lokalisierung kein Datenbank bedürftiges Verfahren, sondern bestimmen die Pose direkt über PoseNet. Die Daten werden entlang eines Flugbahnes aus dem Simulation eines Korridors gesammelt. Hierbei werden sich in der Realitätstreue vom karikaturistisch zu fotorealistisch hin über zu fotorealistisch-texturiert unterscheidende Daten erzeugt. Die besten Ergebnisse konnten die Autoren trainiert mit den Gradienten- und Kantenbilder der karikaturistischen Daten, getestet auf die Gradientenbilder der realen Aufnahmen, erzielen. Bei der Erzeugung von Gradiente- bzw. Kantenbilder gehen einerseits wichtige Informationen im Hinblick auf das Ursprungsbild verloren andererseits bleiben wichtige Informationen wie z. B. die geometrische Struktur erhalten.
Im weiteren Fortgang des Kapitels werden einige grundlegende Themen erläutert. Zuerst wird die Lineare Faltung erklärt und die Verarbeitung eines Bildes über den Sobelfilter zum Gradientenbild ausgeführt. Danach wird ein vertieftes Wissen an CNN vermittelt und anschließend bekannte CNN Modelle näher erläutert.

Lineare Faltung
Sobelfilter
Gradientenbild
Convulutional Neural Network
Convolution Layer
Pooling Layer
Fully Connected Layer
Bekannte CNN Modelle
GoogLeNet
PoseNet




Methodology



Discussion



Fazit




