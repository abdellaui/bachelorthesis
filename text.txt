














 


Abdullah Sahin
Bachelor
Angewandte Informatik
9. September 2019
108016202304
Prof. Dr.-Ing. Markus König
Patrick Herbers, M.Sc.

.....


	


Einleitung

...


Stand der Forschung und Grundlagen

Das vorliegende Kapitel versucht einen Überblick des Forschungsstandes in den unterschiedlichen Aspekten der Arbeit zu verschaffen. Anschließend vermittelt das Kapitel notwendige Grundkenntnisse.

Einführung







// Evt. Einblick über indoor Lokalizierungstechnicken 


Pose Estimation wird in dieser Arbeit als eine Methode der visuellen Lokalisierung (Visual-Based Localization, kurz VBL) betrachtet. VBL beschäftigt sich mit der Bestimmung der Pose (Position + Orientierung) eines visuellen Abfragematerials (z.B. ein RGB-Bild) in einer zuvor bekannten Szene .
Ein naheliegendes Themengebiet der Robotik ist die visuelle Ortswiedererkennung (Visual Place Recognition, kurz VPR). Die visuelle Ortswiedererkennung fokussiert sich auf das Feststellen eines bereits besuchten Ortes und definiert sich aus einer Mapping-, Datenverarbeitungs- und einem Orientierungsmodul. Allgemein lässt sich das Prozess eines VPRs folgend beschreiben. Eine interne Karte bekannter Orte wird durch das Mappingmodul verwaltet. Die Daten werden vom Datenverarbeitungsmodul vorbereitet und anschließend an das Orientierungsmodul übergeben. Daraufhin bestimmt das Orientierungsmodul die Pose und entscheidet mit der immer aktuell gehaltenen Karte, ob ein Ort bereits besucht wurde. Im Vergleich zur VPR versucht die visuelle Lokalisierung eine Pose zu bestimmen und benötigt daher neben den zwei Modulen kein Mappingmodul.


Die rein visuellen Methoden des VBLs unterteilen sich in indirekte und direkte Methoden. Die indirekten Methoden behandeln das Lokalisierungsproblem als eine Bildersuche in einer Datenbank, ähnlich wie das Content Based Image Retrieval Problem. Dabei wird das Abfragebild über eine Ähnlichkeitsfunktion mit den Vergleichsbildern aus der Datenbank abgeglichen. Diese Art von Methoden benötigen eine speicherintensive Bildergalerie (Datenbank) und liefern Ergebnisse bei Fund eines korrespondierenden Bildes. Hingen versuchen die direkten Methoden die Pose über eine Referenzumgebung zu bestimmen und benötigen meist daher keine große Bildergalerie . Es gibt drei Arten der direkten Methoden: 
*[label=*)]
	Abgleichen von Features zu Punktwolken (z.B.)
	Pose Regression mit Tiefenbilder (z.B.)
	Pose Regression nur mit Bildern (z.B.)

Die erste Art von Methoden versucht die Pose zu bestimmen, indem die 2D-3D Korrespondenz über das Abgleichen von Features des Abfragebildes gegen die Deskriptoren der 3D-Punkte hergestellt werden. Diese Vorgehensweise hat Ähnlichkeiten zu den indirekten Methoden und benötigt statt einer Bildergalerie eine repräsentative 3D-Punktwolke der Szene. Die zweite Art von Methoden bestimmt anhand von Tiefenbilder die Pose z.B. über Regression Forests, Randomize Ferns, Coarse-to-Fine Registierung oder Neuronale Netze. Diese Forschungsprojekte liefern mit 3D-Bildern gewünschte Resultate. Die Ergebnisse sind abhängig von 3D-Kameras, die nicht verbreitet sind.

Convolutional Neural Networks (CNN) werden erfolgreich im Bereich des Maschinelles Sehens, wie z.B. bei der Klassifizierung von Bildern sowie bei der Objekterkennung eingesetzt. 
Ein verbreiteter Ansatz beim Entwurf von CNNs ist das Modifizieren der vorhandenen Netzwerkarchitekturen, die z.B. für die Bildklassifizierung angesichts der Wettbewerbe von ImageNet Large Scale Visual Recognition Challenge (ILSVRC) konstruiert wurden. Dieser Ansatz konnte beispielsweise erfolgreich in der Objekterkennung, Objektsegmentierung, semantische Segmentierung  und Tiefenbestimmung verfolgt werden.
Seit Kurzem werden CNNs auch in den Anwendungsgebieten der Lokalisierung verwendet. Zum Beispiel verwenden  CNNs in Bezug auf das Simultaneous-Localization-and-Mapping (SLAM) Problem. schätzen anhand CNNs die relative Pose zweier Kameras. und setzen es im Bereich der visuellen Odometrie ein.


Geleitet von den state-of-the-art Lokalisierungsergebnissen der CNNs stellen den ersten Ansatz zu direkten Posebestimmung nur mit RGB-Bildern vor. PoseNet ist die Modifikation der GoogLeNet Architektur und zweckentfremdet es von der Bildklassifizierung zu einem Pose-Regressor. Trainiert mit einem Datensatz, bestehend aus Paaren von Farbbild und Pose, kann es die sechs Freiheitsgrade der Kamerapose in unbekannten Szenen mittels eines Bildes bestimmen. Dieser Ansatz benötigt weder eine durchsuchbare Bildgalerie noch eine Punktwolke oder Tiefenbilder der Szene. Im Vergleich zu den metrischen Ansätzen wie SLAM oder visuelle Odometrie liefert es eine weniger akkurate Pose. Es bietet jedoch eine hohe Toleranz gegenüber Skalierungs- und Erscheinungsänderungen des Anfragebildes an.


Es gibt mehrere Ansätze, die die Genauigkeit von PoseNet übertreffen.
Einen Fortschritt erhalten die Autoren von PoseNet durch die hier vorgestellte Anpassung ihres Modells an einem Bayessian Neural Network.
Dieselben Autoren erweitern PoseNet mit einer neuen Kostenfunktion unter Berücksichtigung von geometrischen Eigenschaften. und setzen Long-Short-Term-Memory (LSTM) Einheiten ein, um Wissen aus der Korrelation von Bildsequenzen zu gewinnen. und augmentieren den Trainingsdatensatz. stocken den vorhandenen Datensatz auf, indem sie die Bilder künstlich rotieren. erweitern zuerst über ein weiteres CNN den Datensatz um Tiefenbildern. Anschließend simulieren die Autoren RGB-Bilder aus verschiedenen Viewpoints. Im Vergleich zu PoseNet verwenden und eine andere Architektur. 
Das Modell von basiert auf die SqueezeNet Architektur. stellen HourglassNet, basierend auf einem symmetrischen Encoder-Decoder Architektur, vor. und binden zusätzliche Informationen wie z.B. visuelle Odometrie, GPS oder IMU ein. 

Jedes dieser Ansätze benötigen annotierte Trainingsdaten. Für die Erstellung solcher Daten wurden beispielsweise mit entsprechender Hardware ausgerüstete Trolleys, 3D-Kameras oder Structure-from-Motion (SfM) Methoden eingesetzt.


Simulierte 3D-Daten werden in der Literatur oft eingesetzt, um das manuelle Erzeugen und Annotieren von Daten umzugehen.,, und erzeugen ihren Trainingsdaten, indem sie virtuelle Objekte auf reale Hintergrundbildern platzieren. generieren Daten zwecks Personenerkennung und Bestimmung derer körperlicher Pose. Zuvor werden auf den vorhandenen Bildern die körperliche Pose der Personen bestimmt und daran deren 3D Modelle rekonstruiert. Anschließend werden die 3D-Modelle in ihrer Pose variiert auf reale Hintergrundbildern platziert. Die Autoren konnten vergleichbare Ergebnisse zu den vorhandenen Ansätzen mit realen Daten ermitteln. erstellen Daten, um Objekte auf realen Bildern zu detektieren. Von jeder Objektklasse werden 3D-Modelle auf einem Hintergrundbild aus einer Sammlung gelegt. Die Autoren stellen fest, dass das Feintunen mit synthetischen Daten eines Netzwerkes dann zu Abnahme der Akkuratesse führt, wenn das Netzwerk nur für die Detektierung eines Objektes bestimmt ist. Hingegen konnten sie eine Steigung der Ergebnisse beim Trainieren mit simulierten Daten auf vortraniertem Netzwerk mit einer größeren Klassifierungskatalog ermitteln. generieren einen großen Datensatz mit 3D-Modellen, um den Viewpoint von Objekten auf realen Bildern zu bestimmen. Bei dieser Datengenerierung wird jedes virtuelle Objekt auf zufällige Hintergrundbildern positioniert und mit unterschiedlichen Konfigurationen (z.B. Beleuchtung) gerendert. Die Autoren konnten mit der Datenaugmentierung state-of-the-art Viewport-Estimation Methoden zur PASCAL 3D+ Benchmark übertreffen. erstellen künstliche Personen auf Bildern, um beispielsweise den menschlichen Körper in seine Glieder zu segmentieren. Dabei rendern sie zufällige virtuelle Personen mit zufälliger Pose auf beliebige Hintergrundbildern und konnten zeigen, dass die Akkuratesse einiger CNNs durch das Trainieren mit den erzeugten Daten steigt. rendert künstliche Infrarotbilder von Händen sowie Gesichtern zwecks Tiefenerkennung und Segmentierung der Hand in den einzelnen Fingern sowie des Gesichtes in Bereiche aus einem RGB-Bild. Die Autoren konnten konventionelle Methoden über  Helligkeitsabfall übertreffen und vergleichbare Ergebnisse zu den Ansätzen mit einer herkömmlichen 3D-Kamera erzielen. erlernen mit synthetischen Daten den optischen Fluss von Bildsequenzen.  Hierbei werden auf Hintergrundbildern aus einer Sammlung mehrmals bewegte virtuelle Stühle platziert. Die Autoren konnten mit syntethischen Daten state-of-the-art Ansätze über reale Daten übertreffen.

Motiviert von der Datengenerierung über 3D-simulierten Daten stellt einen Ansatz zur bildbasierte Lokalisierung in Gebäuden vor. Dieser Forschungsansatz generiert synthetische Daten aus einem Building-Information-Modeling (BIM). Bei den Daten werden die durch das vortrainierte VGG Netzwerk extrahierte Features als wesentlich erachtet und in einer Datenbank gepflegt. Ein reales Aufnahmebild im Gebäude lässt sich durch den Vergleich der Features lokalisieren. erzeugen ebenso Trainingsdaten aus einem BIM, jedoch verwenden sie zur Lokalisierung keine Datenbank bedürftiges Verfahren, sondern bestimmen die Pose direkt über PoseNet. Die Daten werden entlang einer ca. 30 langen Strecke in der Simulation eines ca. 230 Korridors gesammelt. Hierbei werden sich in der Realitätstreue vom *[label=*)]
	karikaturistisch zu
	fotorealistisch hin über zu
	fotorealistisch-texturiert
 unterscheidende Daten erzeugt.
Traniert mit den unterschiedlichen synthetischen Daten, getestet auf die realen Daten, erzielen die Forscher eine Akkuratesse von
*[label=*)]
	6,25
	5,99
	3,06
 in der Position und  
 *[label=*)]
 	37,16°
 	11,33°
 	12,25°
  in der Orientierung.
Die besten Ergebnisse konnten die Autoren trainiert mit den 
*[label=*)]
		Gradienten- und
	Kantenbilder
 der karikaturistischen Daten, getestet auf die Gradientenbilder der realen Aufnahmen, erzielen. Die Autoren erhalten eine Akkuratesse von 
*[label=*)]
		2,63
	1,88
in der Position und  
 *[label=*)]
 		6,99°
	7,73°
in der Orientierung.

Die vorliegende Arbeit versucht den Ansatz von auf längeren Strecken in größeren Gebäudesimulationen zu untersuchen, worin das PoseNet Modell mit Gradienten- bzw Kantenbilder der synthetischen Daten trainiert und mit Gradientenbilder der realen Daten getestet wird.

Im weiteren Verlauf des Kapitels werden einige grundlegende Themen erläutert. Zuerst werden künstliche neuronale Netze definiert. Danach wird ein elementares Wissen an Convolutional Neural Networks vermittelt und anschließend bekannte CNN Modelle näher erläutert.


Künstliche neuronale Netzwerke
Künstliche neuronale Netze sind ein Forschungsgebiet der künstlichen Intelligenz und imitieren die Beschaffenheit natürlicher neuronale Netze, um komplexe Probleme zu lösen. Inspiriert von ihren biologischen Vorbildern(das Nervensystem eines komplexen Lebewesens; z.B. des Menschen), vernetzen künstliche neuronale Netzwerke (KNN) künstliche Neuronen miteinander. Dabei kann die Verbindung unidirektional (feedforward) oder bidirektional (feedback) sein . 

Bei einem feedforward Netzwerk werden die Daten im Netz immer vorwärts übertragen, hingegen kann ein feedback Netzwerk, auch bekannt als Recurrent Neural Networks, Daten rückwärts, sowie in einer Schleife zum selben Neuron, übergeben. Da feedback Netzwerke keinen Einsatz in dieser Arbeit haben, ist im weiteren Verlauf dieser Arbeit bei einem Netzwerk immer ein feedforward Ansatz gemeint. In diesem Kapitel werden als Nächstes ein künstliches Neuron definiert und anschließend ein feedforward Netzwerk beschrieben. Convolutional Neural Networks sind eine besondere Art von künstlichen neuronalen Netzen und bilden einen sehr wichtigen Bestand dieser Arbeit. Daher werden CNNs ein eigenes Unterkapitel gewidmet, siehe Unterkapitel  .


Künstliches Neuron
Ein einzelnes Neuron erhält einen Inputsignal auf mehreren Kanälen und löst erst ein Signal (output) aus, falls die gewichtete Summe des Inputs einen gewissen Schwellwert erreicht. Abbildung  stellt eine beispielhafte Visualisierung eines künstlichen Neurons dar.

Ein künstliches Neuron mit der Inputgröße  ist mathematisch die nicht-lineare Funktion  mit den Parametern  als Input,  als Gewichtsvektor,  als ein Bias,  als eine nicht-lineare Aktivierungsfunktion:





			Visualisierung eines künstlichen Neurons definiert nach der Gleichung . Dieser Neuron summiert das Produkt des Inputvektors   mit den jeweiligen Gewichten  und addiert einen Bias . Durch die Summe erzeugt die Aktivierungsfunktion  das Output  des Neurons. Abbildung  zeigt ein Beispiel für eine Aktivierungsfunktion. Entnommen aus. 
	
Feedforward Neural Networks
Künstliche Neuronen können zu einem Schicht (layer) zusammengeführt werden. Die Verbindung solcher Schichten bildet ein neuronales Netzwerk.
Bei einem feedforward (fully-connected) Netzwerk übergibt jedes Neuron aus der Schicht  seinen Output  an jedem Neuron der Schicht  weiter. Ebenso sind Neuronen aus der gleichen Schicht untereinander nicht verbunden.
Die Schicht  eines feedforward Netzwerkes operiert somit auf das Ouput  und stellt die nicht-lineare Funktion  dar:



Die erste Schicht eines Netzwerks wird als Input-, die letzte Schicht als Ouput Layer bezeichnet. Alle Schichten dazwischen sind Hidden Layer. Der Ouput Layer liefert zugleich auch das Ergebnis eines Netzwerks, daher haben die Neuronen des Ouput Layers grundsätzlich keine Aktivierungsfunktion.

Die Tiefe (depth) eines Netzwerks ist gegeben durch die Anzahl der Layer (der Input Layer ist ausgeschloßen) und die Breite (width) eines Layers wird durch die Anzahl der Neuronen bestimmt. 
Abbildung  illustriert ein feedforward neuronales Netz als ein azyklischer Graph.

Ziel eines KNNs ist es eine Funktion  zu approximieren, dass einen Input  auf einen Output  abbildet. Durch das Output  kann das Input  klassifiziert oder anhand dessen ein Wert regressiert werden. Sei  eine derartige Funktion, dann besetzt ein KNN die Werte des  Parameters mit eines der besten Approximierung von . Der Parameter  stellt hierbei die Gewichte dar, die erlernt werden sollen. 
Das Lernen ist die strategische Anpassung der Gewichte über Input-Output Paare (Trainingsdaten) und findet grundsätzlich durch ein Backpropagation-Verfahren statt. 

Die Funktion  bildet sich aus den Funktionen der im Netzwerk vorhandenen Schichten (Gleichung ) und kann bei einer Tiefe  repräsentiert werden als die folgende Funktion :
 





			Ein feedforward neuronales Netz mit der Tiefe 3, bestehend aus einem Input Layer der Breite 3, aus zwei Hidden Layer der Breite 4 und einem Output Layer der Breite 1. Mit der Gleichung   lässt sich dieses Netzwerk als die Funktion  mit  darstellen. 
	
Convolutional Neural Networks
Einfache neuronale Netze, wie sie in Abschnitt  beschrieben werden, arbeitet auf einem Inputvektor . Im Vergleich dazu arbeiten CNNs auf einem drei-dimensionalen Inputvolumen . CNNs werden hauptsächlich im Kontext von Bildern eingesetzt, dabei stellt z.B. ein 32  32 RGB-Bild ein Volumen von 32  32  3 dar.

Angefangen mit der LeNet-5 Architektur, setzt sich typischerweise ein Convolutional Neural Network aus einer Sequenz von unterschiedlichen Layer-Arten zusammen. Im weiteren Verlauf dieses Kapitels werden die Arten der Layer beschrieben. Tabelle  gibt eine Übersicht der Layer und ihrer Parameter an.


		Übersicht der Parameter und Hyperparameter der Layer eines Convolutional Neural Networks. Die Parameter werden während der Trainingsphase optimiert und Hyperparameter werden zuvor fest definiert. 
	1.0X X X
	Art des Layers & Parameter & Hyperparameter

		Convolutional Layer & Filter & [tl]
	Filtergröße

	Anzahl der Filter (Tiefe)

	Stride

	Padding

	Aktivierungsfunktion
	

		Pooling Layer &  keine  & [tl]
	Pooling Methode

	Filtergröße

	Stride

	Padding
	

		Fully-Connected Layer & Gewichte & [tl]
	Anzahl der Gewichte

	Aktivierungsfunktion
	

			
Convolutional Layer
Der Convolutional Layer ist der Hauptbestandteil eines CNNs, der die Kombination einer Convolution Operation und einer Aktivierungsfunktion ist.

Die Convolution Operation basiert auf die mathematische Faltung (Convolution) und wird typischerweise in CNNs, mit Verzicht auf die Kommutativität, der anliegenden diskreten Kreuzkorrelation gleichgesetzt.

Sei  ein 2D-Input,  ein 2D-Filter der Größe ,  die Schrittweite (Stride) und  das Output mit der Größe , dann ist die Convolution Operation mathematisch definiert als:

Abbildung  illustriert eine beispielhafte Convolution Operation.

Die Randbehandlung des Inputvolumens wird Padding genannt. Es gibt eine Reihe von Padding Methoden. CNNs setzen grundsätzlich das Zero-Padding Verfahren ein. Das Zero-Padding Verfahren erweitert den Rand  des Inputvolumens um eine beliebige Breite und Höhe mit Nullen. 

Die Werte des 2D-Filters stellen die Gewichte dar und werden in der Trainingsphase optimiert. Ein Convolutional Layer kann aus mehreren 2D-Filter der gleichen Größe bestehen. Die Convolution Operation wird je 2D-Filter unabhängig auf die Schichten des Inputvolumens ausgeführt. Das Output des Convolution Operation wird auch Feature Map genannt. Die Tiefe des Feature Maps ist gegeben durch die Anzahl der 2D-Filter.

Prinzipiell wird das Output des Convolutional Layers durch die Übergabe des Feature Maps an eine Aktivierungsfunktion bestimmt.  Abbildung  stellt eine Aktivierungsfunktion dar.


			Ein Beispiel für eine Aktivierungsfunktion. Die ReLU (rectified liniear unit) Aktivierungsfunktion wird typischerweise in CNNs eingesetzt und ist mathematisch definiert als:  
	


			Ein Beispiel für die  Convolution Operation mit 3 Filtern der Größe 3  3, je einem Stride von 1 und keinem Padding. Ein Filter bewegt sich entlang des gesamten Inputs mit der Schrittweite (Stride) 1 und bildet die Summe der elementweise multiplizierten Werte. Die Summe wird dann im Feature Map an die korrespondierende Position geschrieben. Dieser Vorgang wiederholt sich für jedes Filter. 
	
Pooling Layer
Auf einen Convolutional Layer folgt i.d.R. ein Pooling Layer. Pooling Layer reduzieren die Größe eines Inputvolumens und verringern somit die Anzahl der erlernbaren Parameter. Die Pooling Operation wird auf jeder Schicht der Eingabe ausgeführt. Die meist verbreitete Pooling Methode ist die Max-Pooling. Beim Max-Pooling iteriert ein Filter einer bestimmten Größe mit einer Schrittweite, gegeben durch den Stride, über das Inputvolumen und extrahiert das Maximum im aktuellen Filterbereich. Das Maximum wird für die weitere Berechnung beibehalten und die restlichen Werte verworfen
. In dieser Arbeit wird neben der Max-Pooling Operation auch die Average-Pooling Operation eingesetzt. Das Average-Pooling behält im aktuellen Filterbereich den Durchschnittswert, statt das Maximum.
 
 Im Vergleich zu einem Convolutional Layer wird ein Pooling Layer nur aus Hyperparameter definiert und bleibt daher statisch. Abbildung  zeigt eine beispielhafte Ausführung einer Max-Pooling Operation. Tabelle  listet die Hyperparameter eines Pooling Layers auf.
 
 
			Ein Beispiel für eine Max-Pooling Operation mit einer Filtergröße von 2  2, einem Stride von 2 und keinem Padding. In diesem Beispiel wird der Input in 2  2 Bereiche unterteilt und der Maximum jedes Bereiches als Output berechnet. Es werden die markantesten Werte einer Nachbarschaft behalten und der Rest verworfen. Diese Operation führt zu einer Reduzierung der Inputgröße um den Faktor 2. Entnommen aus.
	


Fully-Connected Layer
Nach einer Periode von Convolution- und Pooling Layer folgt meist ein fully-connected (FC) Layer, auch bekannt als Dense Layer. Dieser Layer folgt dem gleichen Konzept der feedforward Neural Networks, wie in Abschnitt  beschrieben. Das Output dieses Layers wird häufig einer weiteren Aktivierungsfunktion übergeben und prinzipiell wird dadurch das Output des CNNs bestimmt.

Bekannte CNN Modelle
Es existiert eine Menge von bekannten CNN Modellen mit ausgezeichneten Ergebnissen in internationalen Wettbewerben, derartige wie z.B. die ImageNet Large Scale Visual Recognition Challenge (ILSVRC). Dieser Arbeit behandelt nur die Architektur des GoogLeNet Modells und dessen Modifikation PoseNet.

GoogLeNet
GoogLeNet, konstruiert von im Jahr 2014, ist der Sieger des ImageNet Large Scale Visual Recognition Challenge (ILSVRC) 2014 Wettbewerbes. Die ILSVRC stellt ungefähr 1000 Bilder aus 1000 unterschiedlichen Kategorien aus der ImageNet Datensatz bereit. ImageNet ist eine Bildersammlung bestehend aus über 14 Millionen manuell annotierten Bildern in über 20 Tausend unterschiedlichen Kategorien. Die Herausforderung von ILSVRC ist, die 1000 Objekte bestmöglich auf den Bildern zu klassifizieren.

GoogLeNet ist eine besondere Inkarnation des sogenannten Inception Moduls, das zeitgleich mit GoogLeNet vorgestellt wird.
Ein Inception Modul beinhaltet mehrere Convolutional- sowie einen Pooling Layer und verarbeitet das Inputvolumen parallel auf 4 Zweigen. Vor rechenintensiven Convolutional Layer werden zusätzliche 11 dimensionsreduzierende Layer geschaltet. Abschließend werden die Zweige, die ein Volumen mit der gleichen Breite und Höhe produzieren, zu einem Outputvolumen (in die Tiefe) zusammengeführt.  Durch die parallele Berechnung der Zweige und das Einschalten von dimensionsreduzierenden 11 Layer vor rechenintensiven Operationen ermöglicht ein Inception Modul neuronale Netze, bei zunehmender Tiefe und Breite, konstante Rechenkapazität zu verlangen.
Inzwischen wurde das Inception Modul und die gesamte Architektur rundherum weiterentwickelt. Abbildung  stellt das Inception (v1) Modul dar.

Die Architektur von GoogLeNet besteht aus 9 Inception Modulen (je von Tiefe 2) und hat eine gesamte Tiefe von 22 Layer (Anzahl der trainierbaren Layer inkl. Output Layer). Das Ungewöhnliche an der Architektur ist, dass es 3 Output-Zweigen hat. Jedes der Ouput-Zweige produziert ein Klassifizierungsergebnis. Output-Zweig 2  3 sind Hilfszweige, die nur in der Trainingsphase einen Einfluss haben und in der Evaluationsphase verworfen werden. Abbildung  illustriert die GoogLeNet Architektur in Detail.


 
			Darstellung des Inception (v1) Moduls. Das Inputvolumen wird auf 4 Zweige unabhängig verarbeitet. Die Ergebnisse der unabhängigen Operationen haben die gleiche Breite und Höhe. Das Output des Inception Moduls wird durch das Konkatenieren (in der Tiefe) der einzelnen Ergebnisse bestimmt. Die gelben 11 Convolutional Layer dienen zur Dimensionsreduktion. Entnommen aus.
	


	 
			Der architektonische Aufbau des GoogLeNet Models. Das Inception-Block wird in Abbildung  detaillierter dargestellt. Es gibt 3 Ouput-Zweige, die mit einer identisch aufgebauten Output-Knoten enden. Die Werte der Output-Zweige 2  3 haben nur während der Trainingsphase einen Einfluss und werden in der Evaluationsphase verworfen. In der Trainingsphase wird vor dem Output-Knoten 1 ein Dropout von 40 und vor den Output-Knoten 2  3 ein Dropout von 70 angewandt. Jedes Convolutional Layer hat die ReLU (Abbildung ) als Aktivierungsfunktion. Die Local-Response-Normalization ist ein bekanntes Normierungsverfahren und kommt prinzipiell in Verbindung mit der ReLU Aktivierungsfunktion vor. Abbildung basiert auf.
		

PoseNet
PoseNet, vorgestellt von im Jahr 2015, basiert auf die im Kapitel  eingeführte GoogLeNet Architektur. Statt eine Wahrscheinlichkeitsverteilung der Klassifizierungsergebnisse, liefert PoseNet die 6 Freiheitsgrade einer Pose , bestehend aus einem Positionsvektor  und eine Quaternion der Orientierung .

PoseNet modifiziert die GoogLeNet Architektur an den Output-Knoten, dargestellt in Abbildung , wie folgt:

	Jedes der Softmax-Klassifikatoren werden ersetzt durch Regressoren. Dabei wird der Softmax-Activation Layer entfernt und der FC-Layer so modifiziert, dass es einen 7-dimensionalen Vektor((3) für die Position und (4) für die Orientierung) ausgibt.
	Vor dem finalen Regressor des Output-Zweiges 1 wird ein weiteres FC-Layer der Breite 2048 eingefügt.

Abbildung  veranschaulicht die Modifikation von GoogLeNet durch PoseNet.

 
			Veranschaulichung der Modifikation von den Output-Zweigen der GoogLeNet Architektur durch PoseNet. Die Softmax Aktivierungsfunktion sowie das fully-connected Layer mit Anzahl der Klassifizierungskategorien als Breite wurde von allen Output-Modulen entfernt. Jeder Output-Zweig hat einen FC-Regressionsschicht erhalten, das die 6 Freiheitsgrade einer Pose bestimmt. Der Output-Zweig 1 wird zusätzlich mit einer weiteren FC-Layer der Breite 2048 vor dem Regressionsschicht erweitert.
	

Die Autoren stellen zusätzlich eine neue Kostenfunktion, mit den Parametern  als Soll-Wert und  als Ist-Wert, vor:

Der Hyperparameter  soll eine Balance der Kosten zwischen dem Positions- und Orientierungsdiskrepanz darstellen und wird in Gebäuden im Wertebereich zwischen 120 bis 750, sowie außerhalb des Gebäudes zwischen 250 bis 2000, empfohlen. 

Training des Convolutional Neural Networks
Künstliche neuronale Netze bzw. CNNs werden grundsätzlich zuerst aufgabenspezifisch modelliert und anschließend mit entsprechenden Daten trainiert.  In der vorliegenden Arbeit wird als Netzwerk das in Abschnitt  beschriebene PoseNet Model verwendet. Die Arbeit folgt den Ansatz von mit Gradienten- bzw Kantenbilder der synthetischen Daten das Netzwerk zu trainieren, um anschließend mit Gradientenbilder der realen Daten das Netzwerk zu evaluieren. 

Im weiteren Verlauf dieses Kapitel werden die verwendeten Datensätze gelistet, die Erhebung bzw. Generierung der realen bzw. synthetischen Daten erläutert und die Trainingsparameter angegeben. 


Datensätze
IC Gebäude 6. Stock 

Erhebung der realen Daten
In der Literatur wurden die realen Daten einer Zone grundsätzlich entlang einer Strecke aufgenommen. Daher werden für die Aufgabenstellung interessante Aufnahmestrecken in den Gebäudesimulationen festgelegt und anschließend die Aufnahmen so durchgeführt, dass diverse Projekte damit Forschung betreiben können. 

In der Literatur wurden SfM-Methoden eingesetzt, um die Ground-Truth-Daten der realen Aufnahmen zu bestimmen. 
In der vorliegenden Arbeit wird für die Bestimmung der Ground-Truth-Daten sowie die Aufnahme der Bilder zeitgleich zwei unterschiedliche Kameras der Intel Realsense Reihe verwendet. Eine Intel Realsense T265(https://www.intelrealsense.com/tracking-camera-t265/ (abgerufen am: 18.07.2019)) wird eingesetzt, die die Odometrie (Ground-Truth-Daten) mit einer Abweichung von 1  über die SfM von zwei Fischaugenkameras und Inertial Measurement Units (IMU) ermittelt. Zudem wird eine Intel Realsense D435( https://www.intelrealsense.com/depth-camera-d435/ (abgerufen am: 18.07.2019)) eingesetzt, die eine 3D Punktwolke, ein Tiefenbild sowie ein RGB-Bild einer Szene liefert. Die T265 wird über die D435 Kamera montiert (siehe Abbildung ). 

Über das Robot Operating System(https://www.ros.org/about-ros/ (abgerufen am: 18.07.2019)) (ROS) Framework werden die Kameras zeitgleich angesprochen und der Datenfluss der Kameras synchronisiert. Somit beinhaltet jeder Datensatz ein Bild je Fischaugenkamera, ein Tiefenbild, ein RGB-Bild, eine 3D Punktwolke und die dazugehörige Odometrie pro Frame (siehe Abbildung ). Für die vorliegende Arbeit sind nur die Odometrie-Daten der T265 sowie die RGB-Bilder der D435 relevant.



			Hardware für die Aufnahme der realen Daten. Die Intel Realsense T265 ist oberhalb der Intel Realsense D435 montiert. Das Konstrukt kann auf einer universalen Stativschraube befestigt werden.  
	


		[t]0.3
			Odometrie  (T265) + 
 3D Punktwolke (D435)
				[t]0.3
			Fischaugenkamera 1 
 (T265)
				[t]0.3
			Fischaugenkamera 2 
 (T265)
				[t]0.3
			Odometrie  (T265) + 
 3D Punktwolke (D435)
				[t]0.3
			RGB-Bild 
 (D435) 
				[t]0.3
			Tiefenbild 
 (D435) 
			Datensatz pro Frame.   und  visualisieren in unterschiedlichen Prespektiven die von der T265 ermittelte Odometrie und die von der D435 erhaltenen 3D Punktwolke.  und  sind die von der T265 aufgenommenen Fischaugenbilder.  ist das RGB-Bil der D435 und  das dazugehörige Tiefenbild. 
	

Generierung der synthetischen Daten

Die Gebäude werden in Blender(https://www.blender.org (aufgerufen am: 20.07.2019)) Version 2.79b simuliert. Die intrinsischen Daten der D435 RGB-Kamera werden auf die virtuellen Kameras übertragen.

Die Strecke der echten Aufnahmen werden in den Simulationen schwankungslos auf einer konstanten Höhe von 1.70 imitiert. Es wird entlang der Strecke in 0.05 Intervallen und mit einer 10° Neigung in je y- und z-Achse Bilder (siehe Abbildung ) mit korrespondierenden Ground-Truth-Daten aufgenommen.

Insgesamt werden drei synthetische Datensätze je Strecke erzeugt, die sich in ihrer Beschaffenheit von *[label=*)]
	karikaturistisch zu
	karikaturistische Kantenbilder hin über zu
	fotorealistisch
 unterscheiden. Bei der Generierung der a) karikaturistischen und c) fotorealistischen Datensätzen wird die Beleuchtung aus einem Netz von Punktlichtquellen nachgestellt. Die a) karikaturistische und c) fotorealistische Datensätze unterscheiden sich ausschließlich in den Render-Engines.
Für die Erzeugung der b) karikaturistischen Kantenbilder wird eine konstante Beleuchtung verschaffen und die Kanten über Blender markant sichtbar konfiguriert. Der a) karikaturistische Datensatz sowie die b) Kantenbilder werden über die Render-Engine Blender Render und der c) fotorealistischer Datensatz über die Cycles-Engine generiert.




		[t]0.18
			Orginal Pose 
				[t]0.18
			+10° um die y-Achse
			
	[t]0.18
			-10° um die y-Achse
				[t]0.18
			-10° um die z-Achse
				[t]0.18
			+10° um die z-Achse
			
	

Verarbeitung der Daten
// gradienten


Synthetische Bilder, wie sie in   generiert werden, haben keine realitätsnahe Erscheinung, minimale Texture und 


Bei der Erzeugung von Gradientenbilder gehen einerseits wichtige Informationen im Hinblick auf das Ursprungsbild verloren, andererseits bleiben wichtige Informationen wie z.B. die geometrische Struktur erhalten.




		[t]0.48
			reale Aufnahme
			
	[t]0.48
			Gradientenbild von 
		
	[t]0.48
			karikaturistische Simulation
				[t]0.48
			Gradientenbild von 
			[t]0.48
			karikaturistisches Kantenbild
				[t]0.48
			Gradientenbild von 
			[t]0.48
			fotorealistisch Simulation
				[t]0.48
			Gradientenbild von 
		Beispielhafte Bilder für jede Art von Daten und die dazu korrespondierenden Gradientenbilden.
	

Trainingsparameter
Es wird die PoseNet Implementierung in Caffe von den Autoren verwendet. 

Die Gewichte des Netzwerks werden mit den Gewichten eines Models initialisiert, das auf der GoogLeNet Architektur mit dem Places Datensatz trainiert wurde. Anschließend wird mit den erhobenen synthetischen Daten trainiert und mit den realen Daten getestet.
optimizer, beta,
learningrate,
weights decay, anz. data; ...


Ergebnisse
Versuch 1
Versuch 2
Versuch 3


Diskussion





Fazit




